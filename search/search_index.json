{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AlgoFlow Documentation","text":"<p>Web-based quantitative trading platform with automated data pipelines and backtesting infrastructure.</p> <p>Current Status</p> <p>Data pipeline operational - Historical and daily DAGs running in production</p>"},{"location":"#components","title":"Components","text":"<ul> <li>Data Pipeline - Airflow-based securities data ingestion from yfinance</li> <li>API Reference - Complete pipeline and client documentation</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>GitHub Repository</li> <li>Historical DAG</li> <li>Daily DAG</li> </ul>"},{"location":"#local-development","title":"Local Development","text":"<p>Test docs locally with hot reload:</p> <pre><code># Create venv and install dependencies\npython3 -m venv .venv\nsource .venv/bin/activate\npip install -r docs/requirements.txt\n\n# Serve at http://127.0.0.1:8000\nmkdocs serve\n\n# Build static site\nmkdocs build\n</code></pre>"},{"location":"CONTRIBUTING/","title":"How to contribute","text":""},{"location":"CONTRIBUTING/#initial-steps","title":"Initial Steps","text":"<ul> <li>Install python 3.9 or higher (python 3.12 reccomended) from python website or system package manager</li> <li> <p>Check python version: `python3 --version'</p> </li> <li> <p>Call 225-975-5921 if you are unsure or have questions. </p> </li> </ul>"},{"location":"CONTRIBUTING/#1-clone-repo","title":"1. Clone repo","text":"<pre><code>git clone https://github.com/algoflow-quant/algoflow.git\ncd algoflow\n</code></pre>"},{"location":"CONTRIBUTING/#2-install-dependencies","title":"2. Install dependencies","text":""},{"location":"CONTRIBUTING/#frontend-setup","title":"Frontend Setup:","text":"<p>N/A Contact Caden Lund for more</p>"},{"location":"CONTRIBUTING/#backend-setup","title":"Backend Setup:","text":"<p><pre><code>cd backend\npython3 -m venv venv\nsource venv/bin/activate # On windows this is: venv\\Scripts\\activate   \npip install -r requirements.txt\n</code></pre> * Windows users should use WSL for development</p>"},{"location":"CONTRIBUTING/#3-setup-commit-tools","title":"3. Setup commit tools","text":""},{"location":"CONTRIBUTING/#install-commitizen-for-easy-guided-commits","title":"Install commitizen for easy guided commits:","text":"<pre><code># Exit virtual environment\ndeactivate\n# Install pipx for global python packages\nsudo apt install pipx # brew install pipx\n# Install commitizen for guided commits\npipx install commitizen\npipx ensurepath\n\n# Now use the following in place of git commit\ncz commit # Provides interactive helper for formatting\n</code></pre>"},{"location":"CONTRIBUTING/#install-pre-commit-hooks-optional-but-reccommended-catch-commit-errors-before-pushing-to-online-repo","title":"Install pre-commit hooks (Optional but reccommended, catch commit errors before pushing to online repo):","text":"<p>IMPORTANT * You will have to redo commit messages if they are in the wrong format!!! * Note that github actions will prevent you from committing improper messages to the repo. </p> <pre><code># Install pre-commit\npipx install pre-commit\n# Install the git hook\npre-commit install --hook-type commit-msg\n# Now bad commits get blocked locally (faster feedback)\n</code></pre>"},{"location":"CONTRIBUTING/#commit-message-formatting-rules","title":"Commit message formatting rules","text":"<p>All commits must follow the Conventional Commits format:</p> <p><code>type(scope): description</code></p> <p>Can also have a body and footer</p>"},{"location":"CONTRIBUTING/#types-required","title":"Types (Required)","text":"<ul> <li>feat: New feature for the user</li> <li>fix: Bug fix for the user</li> <li>docs: Documentation only changes</li> <li>style: Code style changes (formatting, semicolons, etc)</li> <li>refactor: Code change that neither fixes a bug nor adds a feature</li> <li>perf: Performance improvements</li> <li>test: Adding or updating tests</li> <li>build: Changes to build system or dependencies</li> <li>ci: Changes to CI configuration files and scripts</li> <li>chore: Other changes that don't modify src or test files</li> <li>revert: Reverts a previous commit</li> </ul>"},{"location":"CONTRIBUTING/#scope-optional","title":"Scope (Optional)","text":"<p>The scope should indicate what part of the codebase changed: - <code>backend</code>, <code>frontend</code>, <code>api</code>, <code>db</code>, <code>auth</code>, <code>ui</code>, etc.</p>"},{"location":"CONTRIBUTING/#description-rules","title":"Description Rules","text":"<ul> <li>Use imperative mood (\"add\" not \"added\" or \"adds\")</li> <li>Don't capitalize first letter</li> <li>No period at the end</li> <li>Keep under 72 characters</li> </ul>"},{"location":"CONTRIBUTING/#examples","title":"Examples","text":"<p>Good commits: <pre><code>feat(auth): add OAuth2 login with Google\nfix(api): resolve timeout on stock api\ndocs: update stock database installation instructions in README\n</code></pre></p>"},{"location":"CONTRIBUTING/#other-stuff","title":"Other stuff","text":"<p>For breaking changes, add ! after type/scope and explain in body:</p> <p><code>feat(api)!: change user endpoint response format</code></p> <p>For complex changes, add a body: <pre><code>git commit -m \"feat(backtest): integrate backtesting queue\" -m \"\n- Addded priority queue for backtest nodes\n- Created installation scripts to deploy multiple backtest nodes\n- Added a load balancer to route backtests\n- Update database schema\"\n</code></pre></p>"},{"location":"CONTRIBUTING/#branch-workflow","title":"Branch workflow","text":""},{"location":"CONTRIBUTING/#protected-branches","title":"Protected Branches","text":"<ul> <li>main: Production-ready code (protected, requires PR + review)</li> <li>dev: Integration branch for features (protected, requires PR + review)</li> <li>Never push directly to main or dev!</li> </ul>"},{"location":"CONTRIBUTING/#branch-naming-convention","title":"Branch Naming Convention","text":"<p>Use descriptive branch names with these prefixes:</p> <ul> <li><code>feature/</code> - New features or enhancements</li> <li><code>bugfix/</code> - Bug fixes (non-urgent)</li> <li><code>hotfix/</code> - Urgent production fixes</li> <li><code>docs/</code> - Documentation only changes</li> <li><code>refactor/</code> - Code refactoring (no functionality change)</li> <li><code>test/</code> - Adding or updating tests</li> <li><code>chore/</code> - Maintenance tasks</li> </ul> <p>Examples: - feature/user-authentication - feature/portfolio-analytics - bugfix/chart-rendering-error</p>"},{"location":"CONTRIBUTING/#creating-a-branch","title":"Creating a Branch","text":"<p>Always start from dev: <pre><code>git checkout dev\ngit pull origin dev\ngit checkout -b feature/your-feature-name\n</code></pre></p>"},{"location":"CONTRIBUTING/#working-on-your-branch","title":"Working on your branch","text":"<p>After Changes <pre><code>git add .\ngit commit -m \"feat: add new feature\" \n\n# Keep branch updated with dev\ngit fetch origin\ngit rebase origin/dev  # or merge if you prefer\n\ngit push origin feature/your-feature-name\n</code></pre></p>"},{"location":"CONTRIBUTING/#pull-request-process","title":"Pull Request Process","text":""},{"location":"CONTRIBUTING/#1-push-branch-to-github","title":"1. Push branch to GitHub","text":"<pre><code>git push origin feature/your-feature # Make sure you branch off of dev\n</code></pre>"},{"location":"CONTRIBUTING/#2-create-pull-request","title":"2. Create Pull request","text":"<p>On GitHub: - Click \"Compare &amp; pull request\" button - Base: dev (NEVER directly to main) - Compare: your feature branch</p> <p>PR Title: - Use clear, descriptive title - Example: \"Add portfolio analytics dashboard\"</p>"},{"location":"data_pipeline/overview/","title":"Data Pipeline Overview","text":"<p>Airflow-based pipeline that pulls securities data from yfinance and stores it in PostgreSQL with TimescaleDB.</p>"},{"location":"data_pipeline/overview/#architecture","title":"Architecture","text":"<pre><code>graph TB\n    subgraph \"Airflow DAGs\"\n        H[yfinance_historical_parallel.py]\n        D[yfinance_daily_parallel.py]\n    end\n\n    subgraph \"YfinancePipeline\"\n        S1[scrape_sp500_tickers]\n        S2[scrape_russell3000_tickers]\n        S3[scrape_nasdaq_tickers]\n        V[validate_ticker]\n        SD[scrape_date_range]\n        VO[validate_ohlcv]\n        M[scrape_metadata]\n    end\n\n    subgraph \"YfinanceClient\"\n        IS[insert_security]\n        IO[insert_ohlcv]\n        IM[insert_metadata]\n        GT[get_tickers]\n    end\n\n    subgraph \"Database\"\n        SEC[(securities table)]\n        OHLCV[(ohlcv_data hypertable)]\n        META[(stock_metadata)]\n    end\n\n    H --&gt; S1 &amp; S2 &amp; S3\n    S1 &amp; S2 &amp; S3 --&gt; V\n    V --&gt; IS\n    IS --&gt; SEC\n    SEC --&gt; SD\n    SD --&gt; VO\n    VO --&gt; IO\n    IO --&gt; OHLCV\n    OHLCV --&gt; M\n    M --&gt; IM\n    IM --&gt; META\n\n    D --&gt; GT\n    GT --&gt; SD</code></pre> <p>Architecture Components</p> <p>Three main layers: DAGs orchestrate workflows, Pipeline scrapes/validates data, Client handles database operations</p> <p>The pipeline has three main components:</p> Component Purpose Key Features DAGs Workflow orchestration Airflow <code>.expand()</code> for parallelization, max 20 concurrent tasks Pipeline Data scraping &amp; validation yfinance integration, Great Expectations checks Client Database operations Bulk inserts with <code>execute_values</code>, upsert patterns"},{"location":"data_pipeline/overview/#key-features","title":"Key Features","text":"Feature Implementation Benefit Parallel Execution Airflow <code>.expand()</code> with 20 concurrent tasks Process multiple tickers simultaneously Bulk Inserts <code>psycopg2.extras.execute_values</code> with 1000-row batches 10-100x faster than individual inserts Data Validation 18 Great Expectations checks Catch bad data before database insertion Error Handling Raise exceptions instead of error dicts Airflow handles retries automatically"},{"location":"data_pipeline/overview/#dag-workflows","title":"DAG Workflows","text":"Historical DAGDaily DAG <p>Manual trigger for initial setup or backfills:</p> <ol> <li>Scrape ticker lists from Wikipedia, iShares, NASDAQ</li> <li>Validate each ticker with test download</li> <li>Register valid tickers in <code>securities</code> table</li> <li>Download OHLCV data for date range</li> <li>Validate OHLCV with Great Expectations</li> <li>Bulk insert to <code>ohlcv_data</code> hypertable</li> <li>Download and insert metadata</li> </ol> <p>Auto-runs weekdays at 9:30 PM UTC:</p> <ol> <li>Get tickers from <code>securities</code> table</li> <li>Download latest OHLCV data</li> <li>Validate and insert</li> <li>Update metadata</li> </ol>"},{"location":"data_pipeline/overview/#configuration","title":"Configuration","text":"Variable Example Purpose <code>SEC_MASTER_DB_URL</code> <code>postgresql://user:pass@localhost:5432/sec_master_dev</code> Database connection string"},{"location":"data_pipeline/overview/#deployment","title":"Deployment","text":"<pre><code># Start Airflow and PostgreSQL\ndocker compose -f infrastructure/docker-compose.data-pipeline.yml up -d\n\n# Access Airflow UI\n# http://localhost:8080 (admin/admin)\n</code></pre>"},{"location":"data_pipeline/overview/#performance","title":"Performance","text":"Metric Value Purpose Batch size 1000 rows Bulk insert optimization Parallelism 20 tasks Max concurrent ticker processing Retry strategy 2-3 attempts Exponential backoff for transient failures Rate limiting <code>max_active_tis_per_dag</code> Avoid API throttling"},{"location":"data_pipeline/components/tickers/","title":"YfinanceTickers","text":"<p>Scrapes ticker lists from public sources (S&amp;P 500, Russell 3000, NASDAQ).</p> <p>Component Separation</p> <p>This class handles ticker list scraping only. For data scraping see YfinancePipeline, for validation see YfinanceValidation.</p>"},{"location":"data_pipeline/components/tickers/#sources","title":"Sources","text":"Source Method Count Cleaning S&amp;P 500 Wikipedia table scrape ~500 Replace dots with dashes (BRK.B \u2192 BRK-B) Russell 3000 iShares IWV ETF CSV ~3000 Remove CASH, USD, symbols &gt;5 chars NASDAQ NASDAQ Trader official list ~3000+ Remove test issues"},{"location":"data_pipeline/components/tickers/#how-it-works","title":"How It Works","text":"S&amp;P 500Russell 3000NASDAQ <p>Pulls the Symbol column from Wikipedia's List of S&amp;P 500 companies table. Converts dots to dashes for yfinance compatibility.</p> <p>BRK.B \u2192 BRK-B</p> <p>yfinance uses dashes for class shares, not dots</p> <p>Downloads iShares IWV ETF holdings CSV. The CSV has metadata rows at the top, so we scan for the \"Ticker\" header before parsing. Filters out CASH, USD positions, and symbols &gt;5 characters.</p> <p>CSV Structure</p> <p>First ~10 rows are metadata - scan for \"Ticker\" before parsing</p> <p>Uses the official NASDAQ Trader pipe-delimited file. Filters out test symbols using the \"Test Issue\" flag.</p> <p>Official Source</p> <p>Direct from NASDAQ, updated daily</p>"},{"location":"data_pipeline/components/tickers/#api-reference","title":"API Reference","text":"<p>Ticker list scraping from public sources - Scrapes S&amp;P 500, Russell 3000, NASDAQ ticker lists - Validates and cleans ticker symbols</p> Source code in <code>data_pipeline/sec_data_pipeline/yfinance/yfinance_tickers.py</code> <pre><code>class YfinanceTickers:\n    \"\"\"\n    Ticker list scraping from public sources\n    - Scrapes S&amp;P 500, Russell 3000, NASDAQ ticker lists\n    - Validates and cleans ticker symbols\n    \"\"\"\n\n    # Validation constants\n    MAX_TICKER_LENGTH = 5  # Maximum ticker symbol length\n\n    def __init__(self) -&gt; None:\n        # Add headers to avoid 403 error (Forbidden, for ticker scraping)\n        self.headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n        }\n\n    def scrape_sp500_tickers(self) -&gt; List[str]:\n        \"\"\"\n        Scrapes S&amp;P 500 constituents from Wikipedia's List of S&amp;P 500 companies.\n\n        Pulls the Symbol column from the first table and converts dots to dashes\n        for yfinance compatibility (BRK.B \u2192 BRK-B).\n\n        Returns:\n            List of ~500 ticker symbols\n        \"\"\"\n\n        sp500_url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n\n        # Fetch the HTML with headers\n        response = requests.get(sp500_url, headers=self.headers)\n\n        # Read the table from the HTML\n        tables = pd.read_html(response.text)\n        df = tables[0]\n\n        # Extract tickers from Symbol column\n        tickers = df['Symbol'].tolist()\n\n        # Clean tickers (replace dots with dashes for BRK.B -&gt; BRK-B)\n        tickers = [ticker.replace('.', '-') for ticker in tickers]\n\n        return tickers\n\n    def scrape_russell3000_tickers(self) -&gt; List[str]:\n        \"\"\"\n        Scrapes Russell 3000 tickers from iShares IWV ETF holdings CSV.\n\n        The CSV has metadata rows at the top, so we scan for the \"Ticker\" header\n        before parsing. Filters out CASH, USD positions, and symbols &gt;5 characters.\n\n        Returns:\n            List of ~3000 US equity tickers\n\n        Raises:\n            ValueError: If Ticker column not found in CSV\n        \"\"\"\n\n        # IWV is the iShares Russell 3000 ETF - direct CSV download URL\n        csv_url = \"https://www.ishares.com/us/products/239714/ishares-russell-3000-etf/1467271812596.ajax?fileType=csv&amp;fileName=IWV_holdings&amp;dataType=fund\"\n\n        # Request the csv\n        response = requests.get(csv_url, headers=self.headers)\n\n        # Parse CSV - iShares CSV has metadata in first ~10 rows\n        lines = response.text.split('\\n')\n\n        # Find where the actual data starts (look for \"Ticker\" header)\n        data_start = 0\n        for i, line in enumerate(lines):\n            if 'Ticker' in line:\n                data_start = i\n                break\n\n        # Raise error if the ticker isnt found (only client side exception)\n        if data_start == 0:\n            raise ValueError(\"Could not find ticker data in iShares CSV\")\n\n        # Parse the CSV starting from the data rows (Raw CSV data)\n        csv_data = '\\n'.join(lines[data_start:])\n\n        # Read the raw response from memory\n        df = pd.read_csv(StringIO(csv_data))\n\n        # Extract tickers from the Ticker column\n        tickers = df['Ticker'].dropna().tolist()\n\n        # Clean tickers - remove cash positions and invalid entries\n        tickers = [str(ticker).strip() for ticker in tickers\n                 if ticker and str(ticker).strip()\n                 and not str(ticker).startswith('CASH')\n                 and not str(ticker).startswith('USD')\n                 and len(str(ticker).strip()) &lt;= self.MAX_TICKER_LENGTH]\n\n        return tickers\n\n    def scrape_nasdaq_tickers(self) -&gt; List[str]:\n        \"\"\"\n        Scrapes NASDAQ-listed tickers from the official NASDAQ Trader symbol directory.\n\n        The file is pipe-delimited with a \"Test Issue\" flag that we use to filter out\n        test symbols.\n\n        Returns:\n            List of ~3000+ NASDAQ-listed tickers\n        \"\"\"\n\n        nasdaq_url = \"https://www.nasdaqtrader.com/dynamic/SymDir/nasdaqlisted.txt\"\n\n        # Read the nasdaq url file with | as the separator\n        df = pd.read_csv(nasdaq_url, sep='|')\n\n        # Remove nan values from the data\n        df = df.dropna(subset=['Symbol'])\n\n        # Extract the Symbol column to a list &amp; remove test symbol\n        tickers = df[df['Test Issue'] == 'N']['Symbol'].tolist()\n\n        # Return ticker list\n        return tickers\n</code></pre>"},{"location":"data_pipeline/components/tickers/#sec_data_pipeline.yfinance.yfinance_tickers.YfinanceTickers.scrape_nasdaq_tickers","title":"<code>scrape_nasdaq_tickers()</code>","text":"<p>Scrapes NASDAQ-listed tickers from the official NASDAQ Trader symbol directory.</p> <p>The file is pipe-delimited with a \"Test Issue\" flag that we use to filter out test symbols.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of ~3000+ NASDAQ-listed tickers</p> Source code in <code>data_pipeline/sec_data_pipeline/yfinance/yfinance_tickers.py</code> <pre><code>def scrape_nasdaq_tickers(self) -&gt; List[str]:\n    \"\"\"\n    Scrapes NASDAQ-listed tickers from the official NASDAQ Trader symbol directory.\n\n    The file is pipe-delimited with a \"Test Issue\" flag that we use to filter out\n    test symbols.\n\n    Returns:\n        List of ~3000+ NASDAQ-listed tickers\n    \"\"\"\n\n    nasdaq_url = \"https://www.nasdaqtrader.com/dynamic/SymDir/nasdaqlisted.txt\"\n\n    # Read the nasdaq url file with | as the separator\n    df = pd.read_csv(nasdaq_url, sep='|')\n\n    # Remove nan values from the data\n    df = df.dropna(subset=['Symbol'])\n\n    # Extract the Symbol column to a list &amp; remove test symbol\n    tickers = df[df['Test Issue'] == 'N']['Symbol'].tolist()\n\n    # Return ticker list\n    return tickers\n</code></pre>"},{"location":"data_pipeline/components/tickers/#sec_data_pipeline.yfinance.yfinance_tickers.YfinanceTickers.scrape_russell3000_tickers","title":"<code>scrape_russell3000_tickers()</code>","text":"<p>Scrapes Russell 3000 tickers from iShares IWV ETF holdings CSV.</p> <p>The CSV has metadata rows at the top, so we scan for the \"Ticker\" header before parsing. Filters out CASH, USD positions, and symbols &gt;5 characters.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of ~3000 US equity tickers</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If Ticker column not found in CSV</p> Source code in <code>data_pipeline/sec_data_pipeline/yfinance/yfinance_tickers.py</code> <pre><code>def scrape_russell3000_tickers(self) -&gt; List[str]:\n    \"\"\"\n    Scrapes Russell 3000 tickers from iShares IWV ETF holdings CSV.\n\n    The CSV has metadata rows at the top, so we scan for the \"Ticker\" header\n    before parsing. Filters out CASH, USD positions, and symbols &gt;5 characters.\n\n    Returns:\n        List of ~3000 US equity tickers\n\n    Raises:\n        ValueError: If Ticker column not found in CSV\n    \"\"\"\n\n    # IWV is the iShares Russell 3000 ETF - direct CSV download URL\n    csv_url = \"https://www.ishares.com/us/products/239714/ishares-russell-3000-etf/1467271812596.ajax?fileType=csv&amp;fileName=IWV_holdings&amp;dataType=fund\"\n\n    # Request the csv\n    response = requests.get(csv_url, headers=self.headers)\n\n    # Parse CSV - iShares CSV has metadata in first ~10 rows\n    lines = response.text.split('\\n')\n\n    # Find where the actual data starts (look for \"Ticker\" header)\n    data_start = 0\n    for i, line in enumerate(lines):\n        if 'Ticker' in line:\n            data_start = i\n            break\n\n    # Raise error if the ticker isnt found (only client side exception)\n    if data_start == 0:\n        raise ValueError(\"Could not find ticker data in iShares CSV\")\n\n    # Parse the CSV starting from the data rows (Raw CSV data)\n    csv_data = '\\n'.join(lines[data_start:])\n\n    # Read the raw response from memory\n    df = pd.read_csv(StringIO(csv_data))\n\n    # Extract tickers from the Ticker column\n    tickers = df['Ticker'].dropna().tolist()\n\n    # Clean tickers - remove cash positions and invalid entries\n    tickers = [str(ticker).strip() for ticker in tickers\n             if ticker and str(ticker).strip()\n             and not str(ticker).startswith('CASH')\n             and not str(ticker).startswith('USD')\n             and len(str(ticker).strip()) &lt;= self.MAX_TICKER_LENGTH]\n\n    return tickers\n</code></pre>"},{"location":"data_pipeline/components/tickers/#sec_data_pipeline.yfinance.yfinance_tickers.YfinanceTickers.scrape_sp500_tickers","title":"<code>scrape_sp500_tickers()</code>","text":"<p>Scrapes S&amp;P 500 constituents from Wikipedia's List of S&amp;P 500 companies.</p> <p>Pulls the Symbol column from the first table and converts dots to dashes for yfinance compatibility (BRK.B \u2192 BRK-B).</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of ~500 ticker symbols</p> Source code in <code>data_pipeline/sec_data_pipeline/yfinance/yfinance_tickers.py</code> <pre><code>def scrape_sp500_tickers(self) -&gt; List[str]:\n    \"\"\"\n    Scrapes S&amp;P 500 constituents from Wikipedia's List of S&amp;P 500 companies.\n\n    Pulls the Symbol column from the first table and converts dots to dashes\n    for yfinance compatibility (BRK.B \u2192 BRK-B).\n\n    Returns:\n        List of ~500 ticker symbols\n    \"\"\"\n\n    sp500_url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n\n    # Fetch the HTML with headers\n    response = requests.get(sp500_url, headers=self.headers)\n\n    # Read the table from the HTML\n    tables = pd.read_html(response.text)\n    df = tables[0]\n\n    # Extract tickers from Symbol column\n    tickers = df['Symbol'].tolist()\n\n    # Clean tickers (replace dots with dashes for BRK.B -&gt; BRK-B)\n    tickers = [ticker.replace('.', '-') for ticker in tickers]\n\n    return tickers\n</code></pre>"},{"location":"data_pipeline/components/validation/","title":"YfinanceValidation","text":"<p>Validates ticker symbols and OHLCV data quality using yfinance test downloads and Great Expectations.</p> <p>Component Separation</p> <p>This class handles validation only. For ticker lists see YfinanceTickers, for data scraping see YfinancePipeline.</p>"},{"location":"data_pipeline/components/validation/#validation-checks","title":"Validation Checks","text":"Category Checks Purpose Schema Required columns (Open, High, Low, Close, Volume) Ensure complete data structure Nulls Zero NaN/null tolerance Catch incomplete data Price Logic High \u2265 Low, Open/Close within range, all prices &gt; $0.01 Detect bad data or API errors Data Quality Std dev &gt; 0.01, min 10 rows, unique dates Catch constant values and duplicates Volume Non-negative (0 allowed) Validate trading activity <p>Why 18 Checks?</p> <p>Schema (1) + Nulls (5) + Price bounds (4) + Price logic (4) + Data quality (3) + Unique dates (1) = 18 total checks</p>"},{"location":"data_pipeline/components/validation/#how-it-works","title":"How It Works","text":"<p>Runs 18 Great Expectations checks to catch API glitches, incomplete data, and bad values before they hit the database. Checks cover schema completeness, null values, price logic (High &gt;= Low, Open/Close within bounds), data quality (stddev &gt; 0.01, min 10 rows, unique dates), and reasonable values (prices &gt; $0.01, volume &gt;= 0).</p> <p>Returns a dict with validation results including which checks passed/failed. If the Great Expectations framework itself fails, raises an exception for Airflow to retry.</p> <p>Validation Failures</p> <p>All failures raise exceptions - no silent errors. Airflow retries automatically with exponential backoff.</p>"},{"location":"data_pipeline/components/validation/#constants","title":"Constants","text":"Constant Value Purpose <code>TICKER_VALIDATION_TEST_DAYS</code> 21 Calendar days for ticker test <code>MIN_TRADING_DAYS_FOR_VALIDATION</code> 10 Minimum trading days required <code>MIN_OHLCV_ROWS_FOR_VALIDATION</code> 10 Minimum rows for validation <code>MIN_PRICE_VALUE</code> 0.01 Minimum valid price <code>MIN_STDDEV_VALUE</code> 0.01 Minimum standard deviation"},{"location":"data_pipeline/components/validation/#ticker-validation","title":"Ticker Validation","text":"<p>Each ticker is validated by downloading 21 calendar days of data (about 15 trading days). Valid tickers must return \u226510 trading days back. Catches delisted stocks, bad symbols, and API issues before bulk downloading.</p> <p>Why 21 Days?</p> <p>21 calendar days = ~15 trading days. Accounts for weekends, holidays, and newly listed stocks.</p>"},{"location":"data_pipeline/components/validation/#api-reference","title":"API Reference","text":"<p>Validation for ticker symbols and OHLCV data quality - Ticker validation with test downloads - OHLCV validation with Great Expectations</p> Source code in <code>data_pipeline/sec_data_pipeline/yfinance/yfinance_validation.py</code> <pre><code>class YfinanceValidation:\n    \"\"\"\n    Validation for ticker symbols and OHLCV data quality\n    - Ticker validation with test downloads\n    - OHLCV validation with Great Expectations\n    \"\"\"\n\n    # Validation constants\n    TICKER_VALIDATION_TEST_DAYS = 21  # Calendar days to test ticker validity\n    MIN_TRADING_DAYS_FOR_VALIDATION = 10  # Minimum trading days required for valid ticker\n    MIN_OHLCV_ROWS_FOR_VALIDATION = 10  # Minimum rows required for OHLCV validation\n    MIN_PRICE_VALUE = 0.01  # Minimum valid price (prevents zero/negative prices)\n    MIN_STDDEV_VALUE = 0.01  # Minimum standard deviation (detects constant values)\n\n    def validate_ticker(self, ticker: str, test_days: int = 21) -&gt; bool:\n        \"\"\"\n        Validates a ticker by test-downloading recent data and checking if yfinance\n        returns enough trading days. Catches delisted stocks, bad symbols, and API\n        issues before bulk downloading.\n\n        Downloads 21 calendar days (about 15 trading days) and checks if we got\n        at least 10 trading days back. Threshold accounts for weekends, holidays,\n        and newly listed stocks.\n\n        Args:\n            ticker: Symbol to validate\n            test_days: Calendar days to test (default 21)\n\n        Returns:\n            True if yfinance returned &gt;=10 trading days, False otherwise\n        \"\"\"\n\n        # Calculate date range\n        end_date = date.today()\n        start_date = end_date - timedelta(days=test_days)\n\n        # Download single ticker with explicit auto_adjust\n        data = yf.download(\n            tickers=ticker,\n            start=start_date,\n            end=end_date,\n            progress=False,\n            threads=False,\n            auto_adjust=True  # Explicitly set to avoid warning\n        )\n\n        # Check if we got valid data with minimum trading days\n        if data is not None and not data.empty and len(data) &gt;= self.MIN_TRADING_DAYS_FOR_VALIDATION:\n            return True\n        else:\n            return False\n\n    def validate_ohlcv(self, df: pd.DataFrame, ticker: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Runs 18 Great Expectations checks on OHLCV data to catch API glitches,\n        incomplete data, and bad values before they hit the database.\n\n        Checks cover schema completeness, null values, price logic (High &gt;= Low,\n        Open/Close within bounds), data quality (stddev &gt; 0.01, min 10 rows,\n        unique dates), and reasonable values (prices &gt; $0.01, volume &gt;= 0).\n\n        Args:\n            df: DataFrame with Open, High, Low, Close, Volume columns\n            ticker: Symbol for error messages\n\n        Returns:\n            Dict with validation results:\n                - valid (bool): True if all checks passed\n                - total_checks (int): Should be 18\n                - passed (int): Checks that passed\n                - failed (int): Checks that failed\n                - failed_checks (list): Names of failed expectations\n\n        Raises:\n            Exception: If Great Expectations framework fails\n        \"\"\"\n        # type: ignore - Great Expectations type stubs incomplete\n\n        # Suppress GX progress bars and warnings\n        import warnings\n        import logging\n        import os\n        import sys\n\n        warnings.filterwarnings('ignore')\n\n        # Completely disable all GX logging and progress bars\n        logging.getLogger('great_expectations').disabled = True\n        logging.getLogger('great_expectations.core').disabled = True\n        logging.getLogger('great_expectations.data_context').disabled = True\n\n        # Redirect stderr to suppress tqdm progress bars\n        old_stderr = sys.stderr\n        sys.stderr = open(os.devnull, 'w')\n\n        os.environ['GX_ANALYTICS_ENABLED'] = 'False'\n\n        try:\n            context = gx.get_context()\n            data_source = context.data_sources.add_pandas(name=f\"{ticker}_source\")\n            data_asset = data_source.add_dataframe_asset(name=f\"{ticker}_asset\")\n            batch_def = data_asset.add_batch_definition_whole_dataframe(f\"{ticker}_batch\")\n\n            # Comprehensive expectations for backtest-quality data\n            expectations = [\n                # 1. Schema validation - required columns exist (order doesn't matter)\n                gx.expectations.ExpectTableColumnsToMatchSet(\n                    column_set={\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"}\n                ),\n\n                # 2. Null/NaN validation - zero tolerance\n                gx.expectations.ExpectColumnValuesToNotBeNull(column=\"Open\"),\n                gx.expectations.ExpectColumnValuesToNotBeNull(column=\"High\"),\n                gx.expectations.ExpectColumnValuesToNotBeNull(column=\"Low\"),\n                gx.expectations.ExpectColumnValuesToNotBeNull(column=\"Close\"),\n                gx.expectations.ExpectColumnValuesToNotBeNull(column=\"Volume\"),\n\n                # 3. Positive price validation - no negative or zero prices\n                gx.expectations.ExpectColumnValuesToBeBetween(column=\"Open\", min_value=self.MIN_PRICE_VALUE),\n                gx.expectations.ExpectColumnValuesToBeBetween(column=\"High\", min_value=self.MIN_PRICE_VALUE),\n                gx.expectations.ExpectColumnValuesToBeBetween(column=\"Low\", min_value=self.MIN_PRICE_VALUE),\n                gx.expectations.ExpectColumnValuesToBeBetween(column=\"Close\", min_value=self.MIN_PRICE_VALUE),\n\n                # 4. Volume validation - non-negative only (0 volume is valid)\n                gx.expectations.ExpectColumnValuesToBeBetween(column=\"Volume\", min_value=0),\n\n                # 5. Price logic validation - High &gt;= Low\n                gx.expectations.ExpectColumnPairValuesAToBeGreaterThanB(\n                    column_A=\"High\", column_B=\"Low\", or_equal=True\n                ),\n\n                # 6. Open/Close within High/Low range\n                gx.expectations.ExpectColumnPairValuesAToBeGreaterThanB(\n                    column_A=\"High\", column_B=\"Open\", or_equal=True\n                ),\n                gx.expectations.ExpectColumnPairValuesAToBeGreaterThanB(\n                    column_A=\"Open\", column_B=\"Low\", or_equal=True\n                ),\n                gx.expectations.ExpectColumnPairValuesAToBeGreaterThanB(\n                    column_A=\"High\", column_B=\"Close\", or_equal=True\n                ),\n                gx.expectations.ExpectColumnPairValuesAToBeGreaterThanB(\n                    column_A=\"Close\", column_B=\"Low\", or_equal=True\n                ),\n\n                # 7. Data variability - no constant values (stddev &gt; 0)\n                gx.expectations.ExpectColumnStdevToBeBetween(column=\"Close\", min_value=self.MIN_STDDEV_VALUE),\n                gx.expectations.ExpectColumnStdevToBeBetween(column=\"Volume\", min_value=self.MIN_STDDEV_VALUE),\n\n                # 8. Minimum row count - at least 10 trading days\n                gx.expectations.ExpectTableRowCountToBeBetween(min_value=self.MIN_OHLCV_ROWS_FOR_VALIDATION),\n\n                # 9. Unique dates - no duplicate timestamps\n                gx.expectations.ExpectColumnValuesToBeUnique(column=\"Date\") if \"Date\" in df.columns else None,\n            ]\n\n            # Remove None values (for conditional expectations)\n            expectations = [e for e in expectations if e is not None]\n\n            # Run all validations and collect results\n            batch = batch_def.get_batch(batch_parameters={\"dataframe\": df})\n\n            results = []\n            failed_checks = []\n\n            for expectation in expectations:\n                result = batch.validate(expectation)\n                results.append(result)\n\n                if not result.success:\n                    # Get expectation type for better logging\n                    expectation_type = type(expectation).__name__\n                    failed_checks.append(expectation_type)\n\n            passed = sum(1 for r in results if r.success)\n            failed = len(results) - passed\n\n            # Restore stderr\n            sys.stderr = old_stderr\n\n            return {\n                'valid': failed == 0,\n                'total_checks': len(results),\n                'passed': passed,\n                'failed': failed,\n                'failed_checks': failed_checks\n            }\n\n        except Exception as e:\n            # Restore stderr\n            sys.stderr = old_stderr\n\n            # If validation setup fails, raise exception for Airflow to retry\n            raise Exception(f\"OHLCV validation framework failed for {ticker}: {str(e)}\") from e\n</code></pre>"},{"location":"data_pipeline/components/validation/#sec_data_pipeline.yfinance.yfinance_validation.YfinanceValidation.validate_ohlcv","title":"<code>validate_ohlcv(df, ticker)</code>","text":"<p>Runs 18 Great Expectations checks on OHLCV data to catch API glitches, incomplete data, and bad values before they hit the database.</p> <p>Checks cover schema completeness, null values, price logic (High &gt;= Low, Open/Close within bounds), data quality (stddev &gt; 0.01, min 10 rows, unique dates), and reasonable values (prices &gt; $0.01, volume &gt;= 0).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with Open, High, Low, Close, Volume columns</p> required <code>ticker</code> <code>str</code> <p>Symbol for error messages</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with validation results: - valid (bool): True if all checks passed - total_checks (int): Should be 18 - passed (int): Checks that passed - failed (int): Checks that failed - failed_checks (list): Names of failed expectations</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If Great Expectations framework fails</p> Source code in <code>data_pipeline/sec_data_pipeline/yfinance/yfinance_validation.py</code> <pre><code>def validate_ohlcv(self, df: pd.DataFrame, ticker: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Runs 18 Great Expectations checks on OHLCV data to catch API glitches,\n    incomplete data, and bad values before they hit the database.\n\n    Checks cover schema completeness, null values, price logic (High &gt;= Low,\n    Open/Close within bounds), data quality (stddev &gt; 0.01, min 10 rows,\n    unique dates), and reasonable values (prices &gt; $0.01, volume &gt;= 0).\n\n    Args:\n        df: DataFrame with Open, High, Low, Close, Volume columns\n        ticker: Symbol for error messages\n\n    Returns:\n        Dict with validation results:\n            - valid (bool): True if all checks passed\n            - total_checks (int): Should be 18\n            - passed (int): Checks that passed\n            - failed (int): Checks that failed\n            - failed_checks (list): Names of failed expectations\n\n    Raises:\n        Exception: If Great Expectations framework fails\n    \"\"\"\n    # type: ignore - Great Expectations type stubs incomplete\n\n    # Suppress GX progress bars and warnings\n    import warnings\n    import logging\n    import os\n    import sys\n\n    warnings.filterwarnings('ignore')\n\n    # Completely disable all GX logging and progress bars\n    logging.getLogger('great_expectations').disabled = True\n    logging.getLogger('great_expectations.core').disabled = True\n    logging.getLogger('great_expectations.data_context').disabled = True\n\n    # Redirect stderr to suppress tqdm progress bars\n    old_stderr = sys.stderr\n    sys.stderr = open(os.devnull, 'w')\n\n    os.environ['GX_ANALYTICS_ENABLED'] = 'False'\n\n    try:\n        context = gx.get_context()\n        data_source = context.data_sources.add_pandas(name=f\"{ticker}_source\")\n        data_asset = data_source.add_dataframe_asset(name=f\"{ticker}_asset\")\n        batch_def = data_asset.add_batch_definition_whole_dataframe(f\"{ticker}_batch\")\n\n        # Comprehensive expectations for backtest-quality data\n        expectations = [\n            # 1. Schema validation - required columns exist (order doesn't matter)\n            gx.expectations.ExpectTableColumnsToMatchSet(\n                column_set={\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"}\n            ),\n\n            # 2. Null/NaN validation - zero tolerance\n            gx.expectations.ExpectColumnValuesToNotBeNull(column=\"Open\"),\n            gx.expectations.ExpectColumnValuesToNotBeNull(column=\"High\"),\n            gx.expectations.ExpectColumnValuesToNotBeNull(column=\"Low\"),\n            gx.expectations.ExpectColumnValuesToNotBeNull(column=\"Close\"),\n            gx.expectations.ExpectColumnValuesToNotBeNull(column=\"Volume\"),\n\n            # 3. Positive price validation - no negative or zero prices\n            gx.expectations.ExpectColumnValuesToBeBetween(column=\"Open\", min_value=self.MIN_PRICE_VALUE),\n            gx.expectations.ExpectColumnValuesToBeBetween(column=\"High\", min_value=self.MIN_PRICE_VALUE),\n            gx.expectations.ExpectColumnValuesToBeBetween(column=\"Low\", min_value=self.MIN_PRICE_VALUE),\n            gx.expectations.ExpectColumnValuesToBeBetween(column=\"Close\", min_value=self.MIN_PRICE_VALUE),\n\n            # 4. Volume validation - non-negative only (0 volume is valid)\n            gx.expectations.ExpectColumnValuesToBeBetween(column=\"Volume\", min_value=0),\n\n            # 5. Price logic validation - High &gt;= Low\n            gx.expectations.ExpectColumnPairValuesAToBeGreaterThanB(\n                column_A=\"High\", column_B=\"Low\", or_equal=True\n            ),\n\n            # 6. Open/Close within High/Low range\n            gx.expectations.ExpectColumnPairValuesAToBeGreaterThanB(\n                column_A=\"High\", column_B=\"Open\", or_equal=True\n            ),\n            gx.expectations.ExpectColumnPairValuesAToBeGreaterThanB(\n                column_A=\"Open\", column_B=\"Low\", or_equal=True\n            ),\n            gx.expectations.ExpectColumnPairValuesAToBeGreaterThanB(\n                column_A=\"High\", column_B=\"Close\", or_equal=True\n            ),\n            gx.expectations.ExpectColumnPairValuesAToBeGreaterThanB(\n                column_A=\"Close\", column_B=\"Low\", or_equal=True\n            ),\n\n            # 7. Data variability - no constant values (stddev &gt; 0)\n            gx.expectations.ExpectColumnStdevToBeBetween(column=\"Close\", min_value=self.MIN_STDDEV_VALUE),\n            gx.expectations.ExpectColumnStdevToBeBetween(column=\"Volume\", min_value=self.MIN_STDDEV_VALUE),\n\n            # 8. Minimum row count - at least 10 trading days\n            gx.expectations.ExpectTableRowCountToBeBetween(min_value=self.MIN_OHLCV_ROWS_FOR_VALIDATION),\n\n            # 9. Unique dates - no duplicate timestamps\n            gx.expectations.ExpectColumnValuesToBeUnique(column=\"Date\") if \"Date\" in df.columns else None,\n        ]\n\n        # Remove None values (for conditional expectations)\n        expectations = [e for e in expectations if e is not None]\n\n        # Run all validations and collect results\n        batch = batch_def.get_batch(batch_parameters={\"dataframe\": df})\n\n        results = []\n        failed_checks = []\n\n        for expectation in expectations:\n            result = batch.validate(expectation)\n            results.append(result)\n\n            if not result.success:\n                # Get expectation type for better logging\n                expectation_type = type(expectation).__name__\n                failed_checks.append(expectation_type)\n\n        passed = sum(1 for r in results if r.success)\n        failed = len(results) - passed\n\n        # Restore stderr\n        sys.stderr = old_stderr\n\n        return {\n            'valid': failed == 0,\n            'total_checks': len(results),\n            'passed': passed,\n            'failed': failed,\n            'failed_checks': failed_checks\n        }\n\n    except Exception as e:\n        # Restore stderr\n        sys.stderr = old_stderr\n\n        # If validation setup fails, raise exception for Airflow to retry\n        raise Exception(f\"OHLCV validation framework failed for {ticker}: {str(e)}\") from e\n</code></pre>"},{"location":"data_pipeline/components/validation/#sec_data_pipeline.yfinance.yfinance_validation.YfinanceValidation.validate_ticker","title":"<code>validate_ticker(ticker, test_days=21)</code>","text":"<p>Validates a ticker by test-downloading recent data and checking if yfinance returns enough trading days. Catches delisted stocks, bad symbols, and API issues before bulk downloading.</p> <p>Downloads 21 calendar days (about 15 trading days) and checks if we got at least 10 trading days back. Threshold accounts for weekends, holidays, and newly listed stocks.</p> <p>Parameters:</p> Name Type Description Default <code>ticker</code> <code>str</code> <p>Symbol to validate</p> required <code>test_days</code> <code>int</code> <p>Calendar days to test (default 21)</p> <code>21</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if yfinance returned &gt;=10 trading days, False otherwise</p> Source code in <code>data_pipeline/sec_data_pipeline/yfinance/yfinance_validation.py</code> <pre><code>def validate_ticker(self, ticker: str, test_days: int = 21) -&gt; bool:\n    \"\"\"\n    Validates a ticker by test-downloading recent data and checking if yfinance\n    returns enough trading days. Catches delisted stocks, bad symbols, and API\n    issues before bulk downloading.\n\n    Downloads 21 calendar days (about 15 trading days) and checks if we got\n    at least 10 trading days back. Threshold accounts for weekends, holidays,\n    and newly listed stocks.\n\n    Args:\n        ticker: Symbol to validate\n        test_days: Calendar days to test (default 21)\n\n    Returns:\n        True if yfinance returned &gt;=10 trading days, False otherwise\n    \"\"\"\n\n    # Calculate date range\n    end_date = date.today()\n    start_date = end_date - timedelta(days=test_days)\n\n    # Download single ticker with explicit auto_adjust\n    data = yf.download(\n        tickers=ticker,\n        start=start_date,\n        end=end_date,\n        progress=False,\n        threads=False,\n        auto_adjust=True  # Explicitly set to avoid warning\n    )\n\n    # Check if we got valid data with minimum trading days\n    if data is not None and not data.empty and len(data) &gt;= self.MIN_TRADING_DAYS_FOR_VALIDATION:\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"data_pipeline/components/yfinance_client/","title":"YfinanceClient","text":"<p>Database client for sec_master operations. Main performance win is bulk inserts with psycopg2 execute_values.</p>"},{"location":"data_pipeline/components/yfinance_client/#quick-start","title":"Quick Start","text":"Basic UsageGet Tickers <pre><code>from sec_master_db.clients.yfinance_client import YfinanceClient\nfrom utils.database import get_database_url\n\nclient = YfinanceClient(get_database_url())\n\n# Register security\nclient.insert_security('AAPL', ['sp500', 'nasdaq'])\n\n# Bulk insert OHLCV (252 rows in &lt;1 second)\nresult = client.insert_ohlcv('AAPL', df)\n\n# Insert metadata\nclient.insert_metadata('AAPL', metadata_dict)\n</code></pre> <pre><code># Get all tickers\nall_tickers = client.get_tickers()\n\n# Get S&amp;P 500 tickers only\nsp500 = client.get_tickers(['sp500'])\n\n# Get multiple groupings\ncombined = client.get_tickers(['sp500', 'nasdaq'])\n</code></pre>"},{"location":"data_pipeline/components/yfinance_client/#key-features","title":"Key Features","text":"Feature Implementation Benefit Bulk inserts <code>execute_values</code> with 1000-row batches 10-100x faster than individual inserts Upsert pattern <code>ON CONFLICT UPDATE/DO NOTHING</code> Safe to re-run, idempotent operations Session management Create/use/close per operation Automatic cleanup, no connection leaks Error handling Rollback + raise with context Clean failures, Airflow can retry"},{"location":"data_pipeline/components/yfinance_client/#database-schema","title":"Database Schema","text":"Table Purpose Key Constraints <code>security_master.securities</code> Ticker registry with groupings UNIQUE (ticker, provider) <code>yfinance.ohlcv_data</code> Price data (TimescaleDB hypertable) UNIQUE (security_id, date) <code>yfinance.stock_metadata</code> 50+ fundamental fields UNIQUE (security_id, date_scraped) <p>Performance</p> <p>TimescaleDB hypertable partitioning makes time-series queries 10-100x faster than regular PostgreSQL tables</p>"},{"location":"data_pipeline/components/yfinance_client/#api-reference","title":"API Reference","text":"Source code in <code>data_pipeline/sec_master_db/clients/yfinance_client.py</code> <pre><code>class YfinanceClient:\n    def __init__(self, db_url: str):\n        \"\"\"\n        Initialize YfinanceClient for database operations.\n\n        Args:\n            db_url: PostgreSQL connection string. If None, uses SEC_MASTER_DB_URL_PROD env var\n                   or defaults to local dev database.\n\n        Example:\n            ```python\n            client = YfinanceClient(\"postgresql://user:pass@localhost:5432/sec_master_dev\")\n            ```\n        \"\"\"\n        # Set up connection\n        self.engine = create_engine(db_url)\n        self.Session = sessionmaker(bind=self.engine)\n\n\n    # Security management methods    \n    def insert_security(self, ticker: str, groupings: List[str], provider: str = 'yfinance') -&gt; Dict[str, Any]:\n        \"\"\"\n        Insert a ticker symbol into the security_master.securities table.\n\n        Args:\n            ticker: Ticker symbol (e.g., 'AAPL')\n            groupings: List of grouping tags (e.g., ['sp500', 'tech', 'large-cap'])\n            provider: Data provider name (default: 'yfinance')\n\n        Returns:\n            Dict with keys: success (bool), ticker (str), rows_affected (int), message (str)\n\n        Raises:\n            Exception: If database insertion fails\n\n        Example:\n            ```python\n            result = client.insert_security('AAPL', ['sp500', 'nasdaq'])\n            if result['rows_affected'] &gt; 0:\n                print(\"Inserted successfully\")\n            ```\n\n        Note:\n            Uses ON CONFLICT DO NOTHING, so duplicate tickers return rowcount=0.\n        \"\"\"\n\n        session = self.Session()\n\n        try:\n\n            # Insert a security\n            query = text(\"\"\"\n                INSERT INTO security_master.securities (ticker, provider, groupings, created_at)\n                VALUES (:ticker, :provider, :groupings, NOW())\n                ON CONFLICT (ticker, provider)\n                DO NOTHING\n            \"\"\")\n\n            # Execute query\n            result = session.execute(query, {'ticker': ticker, 'provider': provider, 'groupings': groupings})\n\n            # Get the number of affected rows\n            rows = getattr(result, 'rowcount', 0)\n\n            # Commit saves DB changes\n            session.commit()\n\n            # Return a formatted summary dict\n            return {\n                'success': True,\n                'ticker': ticker,\n                'rows_affected': rows,\n                'message': f\"Inserted {ticker}\" if rows &gt; 0 else f\"{ticker} already exists\"\n            }\n\n        # Catch Error\n        except Exception as e:\n            session.rollback()  # Important: rollback on error so we dont commit partial data\n            logger.error(f\"Failed to insert security {ticker}: {e}\")\n            raise Exception(f\"Security insertion failed for {ticker}: {str(e)}\") from e\n        finally:\n            session.close()\n\n    def get_security_id(self, ticker: str, provider: str = 'yfinance') -&gt; Optional[int]:\n        \"\"\"\n        Retrieve the unique security_id for a ticker symbol.\n\n        Args:\n            ticker: Ticker symbol (e.g., 'AAPL')\n            provider: Data provider name (default: 'yfinance')\n\n        Returns:\n            int: The security_id if found, None if ticker doesn't exist\n\n        Example:\n            ```python\n            security_id = client.get_security_id('AAPL')\n            if security_id:\n                print(f\"AAPL has ID: {security_id}\")\n            ```\n\n        Note:\n            This ID is used as foreign key in OHLCV and metadata tables.\n        \"\"\"\n        session = self.Session()\n\n        try:\n            query = text(\"\"\"\n                SELECT security_id\n                FROM security_master.securities\n                WHERE ticker = :ticker AND provider = :provider\n            \"\"\")\n\n            # Execute SQL query\n            result = session.execute(query, {'ticker': ticker, 'provider': provider})\n\n            # Get the first row\n            row = result.fetchone()\n\n            # Return the security id\n            if not row:\n                raise ValueError(f\"Security ID not found for ticker {ticker} with provider {provider}\")\n            return row[0]\n\n        except Exception as e:\n            logger.error(f\"Failed to get security_id for {ticker}: {e}\")\n            raise Exception(f\"Failed to retrieve security_id for {ticker}: {str(e)}\") from e\n        finally:\n            session.close()\n\n    # Yfinance Schema storage\n    def insert_ohlcv(self, ticker: str, data: pd.DataFrame) -&gt; Dict[str, Any]:\n        \"\"\"\n        Insert OHLCV (Open, High, Low, Close, Volume) data for a single ticker.\n\n        Args:\n            ticker: Ticker symbol (must exist in securities table)\n            data: DataFrame with columns: Date (index), Open, High, Low, Close, Volume\n                  Date can be either index or column.\n\n        Returns:\n            Dict with keys: success (bool), ticker (str), rows_affected (int), message (str)\n\n        Raises:\n            Exception: If ticker not found or insertion fails\n\n        Example:\n            ```python\n            import yfinance as yf\n            df = yf.download('AAPL', start='2024-01-01', end='2024-12-31')\n            result = client.insert_ohlcv('AAPL', df)\n            print(f\"Inserted/updated {result['rows_affected']} rows\")\n            ```\n\n        Note:\n            Uses ON CONFLICT UPDATE to handle duplicate dates (updates existing records).\n        \"\"\"\n\n        session = self.Session()\n\n        try:\n            # Get security id \n            security_id = self.get_security_id(ticker)\n\n            # Prepare data as tuples for maximum performance\n            records = data.reset_index()\n\n            # Create list of tuples for execute_values (MUCH faster than execute_many)\n            insert_data = [\n                (\n                    security_id,\n                    row['Date'],\n                    float(row['Open']),\n                    float(row['High']),\n                    float(row['Low']),\n                    float(row['Close']),\n                    int(row['Volume'])\n                )\n                for _, row in records.iterrows()\n            ]\n\n            # Get the raw connection from SQLAlchemy session\n            raw_conn = session.connection().connection\n            cursor = raw_conn.cursor()\n\n            # Use execute_values with ON CONFLICT for bulk upsert (SUPER FAST!)\n            execute_values(\n                cursor,\n                \"\"\"\n                INSERT INTO yfinance.ohlcv_data\n                (security_id, date, open, high, low, close, volume)\n                VALUES %s\n                ON CONFLICT (security_id, date)\n                DO UPDATE SET\n                    open = EXCLUDED.open,\n                    high = EXCLUDED.high,\n                    low = EXCLUDED.low,\n                    close = EXCLUDED.close,\n                    volume = EXCLUDED.volume\n                \"\"\",\n                insert_data,\n                template=None,  # Use default template\n                page_size=1000  # Process 1000 rows at a time\n            )\n\n            # Get rows affected from cursor\n            rows = cursor.rowcount # row update/insert count\n\n            # Commit the changes\n            raw_conn.commit()\n            cursor.close()\n            return {\n                'success': True,\n                'ticker': ticker,\n                'rows_affected': rows,\n                'message': f\"Insert OHLCV for {ticker}\" if rows &gt; 0 else f\"{ticker} already in OHLCV DB\"\n            }\n\n        except Exception as e:\n            # Rollback both the raw connection and session\n            try:\n                raw_conn.rollback()\n            except:\n                pass\n            session.rollback()\n            logger.error(f\"Failed to insert OHLCV data for {ticker}: {e}\")\n            raise Exception(f\"OHLCV insertion failed for {ticker}: {str(e)}\") from e\n\n        finally:\n            session.close()\n\n\n\n\n    def insert_metadata(self, ticker: str, metadata: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Insert comprehensive financial metadata for a ticker.\n\n        Args:\n            ticker: Ticker symbol (must exist in securities table)\n            metadata: Dictionary containing financial metrics. Expected keys include:\n                     - Company info: company_name, exchange, sector, industry, country\n                     - Valuation: market_cap, enterprise_value, price_to_book, forward_pe\n                     - Financials: gross_margin, operating_margin, profit_margin, debt_to_equity\n                     - Performance: beta, 52_week_high, 52_week_low, average_volume\n                     - And many more (see schema for full list)\n\n        Returns:\n            Dict with keys: success (bool), ticker (str), rows_affected (int), message (str)\n\n        Raises:\n            Exception: If ticker not found or insertion fails\n\n        Example:\n            ```python\n            metadata = pipeline.scrape_metadata('AAPL')\n            result = client.insert_metadata('AAPL', metadata)\n            print(f\"Rows affected: {result['rows_affected']}\")\n            ```\n\n        Note:\n            Missing keys are stored as NULL. Uses ON CONFLICT UPDATE for existing records.\n        \"\"\"\n        session = self.Session()\n\n        try:\n            # Get security_id\n            security_id = self.get_security_id(ticker)\n\n            if not security_id:\n                raise Exception(f\"No security id for ticker: {ticker}\")\n\n            # Build the INSERT query with all columns\n            query = text(\"\"\"\n                INSERT INTO yfinance.stock_metadata (\n                    security_id, date_scraped,\n                    -- Company Basic Info\n                    company_name, exchange, country, sector, industry,\n                    market_cap, enterprise_value, shares_outstanding, float_shares,\n                    -- Valuation Metrics\n                    price_to_book, forward_pe, ev_to_ebitda, ev_to_revenue, price_to_sales,\n                    -- Profitability &amp; Quality\n                    gross_margin, operating_margin, profit_margin,\n                    return_on_equity, return_on_assets, free_cash_flow_yield,\n                    -- Growth Metrics\n                    revenue_growth_yoy, revenue_per_share,\n                    -- Financial Health\n                    debt_to_equity, current_ratio, quick_ratio,\n                    total_cash, total_debt, total_cash_per_share, book_value,\n                    -- Cash Flow\n                    operating_cash_flow, free_cash_flow,\n                    -- Dividends\n                    payout_ratio,\n                    -- Short Interest &amp; Ownership\n                    short_percent_of_float, short_ratio, shares_short,\n                    shares_percent_shares_out, held_percent_institutions, held_percent_insiders,\n                    -- Analyst Coverage\n                    target_mean_price, target_price_upside, number_of_analysts, recommendation_key,\n                    -- Market Performance\n                    beta, fifty_two_week_high, fifty_two_week_low,\n                    fifty_two_week_change, sp500_52_week_change,\n                    fifty_day_average, two_hundred_day_average,\n                    -- Trading Volume\n                    average_volume, average_volume_10days, regular_market_volume,\n                    -- Metadata\n                    data_source\n                )\n                VALUES (\n                    :security_id, :date_scraped,\n                    -- Company Basic Info\n                    :company_name, :exchange, :country, :sector, :industry,\n                    :market_cap, :enterprise_value, :shares_outstanding, :float_shares,\n                    -- Valuation Metrics\n                    :price_to_book, :forward_pe, :ev_to_ebitda, :ev_to_revenue, :price_to_sales,\n                    -- Profitability &amp; Quality\n                    :gross_margin, :operating_margin, :profit_margin,\n                    :return_on_equity, :return_on_assets, :free_cash_flow_yield,\n                    -- Growth Metrics\n                    :revenue_growth_yoy, :revenue_per_share,\n                    -- Financial Health\n                    :debt_to_equity, :current_ratio, :quick_ratio,\n                    :total_cash, :total_debt, :total_cash_per_share, :book_value,\n                    -- Cash Flow\n                    :operating_cash_flow, :free_cash_flow,\n                    -- Dividends\n                    :payout_ratio,\n                    -- Short Interest &amp; Ownership\n                    :short_percent_of_float, :short_ratio, :shares_short,\n                    :shares_percent_shares_out, :held_percent_institutions, :held_percent_insiders,\n                    -- Analyst Coverage\n                    :target_mean_price, :target_price_upside, :number_of_analysts, :recommendation_key,\n                    -- Market Performance\n                    :beta, :fifty_two_week_high, :fifty_two_week_low,\n                    :fifty_two_week_change, :sp500_52_week_change,\n                    :fifty_day_average, :two_hundred_day_average,\n                    -- Trading Volume\n                    :average_volume, :average_volume_10days, :regular_market_volume,\n                    -- Metadata\n                    :data_source\n                )\n                ON CONFLICT (security_id, date_scraped)\n                DO UPDATE SET\n                    company_name = EXCLUDED.company_name,\n                    exchange = EXCLUDED.exchange,\n                    country = EXCLUDED.country,\n                    sector = EXCLUDED.sector,\n                    industry = EXCLUDED.industry,\n                    market_cap = EXCLUDED.market_cap,\n                    enterprise_value = EXCLUDED.enterprise_value,\n                    shares_outstanding = EXCLUDED.shares_outstanding,\n                    float_shares = EXCLUDED.float_shares,\n                    price_to_book = EXCLUDED.price_to_book,\n                    forward_pe = EXCLUDED.forward_pe,\n                    ev_to_ebitda = EXCLUDED.ev_to_ebitda,\n                    ev_to_revenue = EXCLUDED.ev_to_revenue,\n                    price_to_sales = EXCLUDED.price_to_sales,\n                    gross_margin = EXCLUDED.gross_margin,\n                    operating_margin = EXCLUDED.operating_margin,\n                    profit_margin = EXCLUDED.profit_margin,\n                    return_on_equity = EXCLUDED.return_on_equity,\n                    return_on_assets = EXCLUDED.return_on_assets,\n                    free_cash_flow_yield = EXCLUDED.free_cash_flow_yield,\n                    revenue_growth_yoy = EXCLUDED.revenue_growth_yoy,\n                    revenue_per_share = EXCLUDED.revenue_per_share,\n                    debt_to_equity = EXCLUDED.debt_to_equity,\n                    current_ratio = EXCLUDED.current_ratio,\n                    quick_ratio = EXCLUDED.quick_ratio,\n                    total_cash = EXCLUDED.total_cash,\n                    total_debt = EXCLUDED.total_debt,\n                    total_cash_per_share = EXCLUDED.total_cash_per_share,\n                    book_value = EXCLUDED.book_value,\n                    operating_cash_flow = EXCLUDED.operating_cash_flow,\n                    free_cash_flow = EXCLUDED.free_cash_flow,\n                    payout_ratio = EXCLUDED.payout_ratio,\n                    short_percent_of_float = EXCLUDED.short_percent_of_float,\n                    short_ratio = EXCLUDED.short_ratio,\n                    shares_short = EXCLUDED.shares_short,\n                    shares_percent_shares_out = EXCLUDED.shares_percent_shares_out,\n                    held_percent_institutions = EXCLUDED.held_percent_institutions,\n                    held_percent_insiders = EXCLUDED.held_percent_insiders,\n                    target_mean_price = EXCLUDED.target_mean_price,\n                    target_price_upside = EXCLUDED.target_price_upside,\n                    number_of_analysts = EXCLUDED.number_of_analysts,\n                    recommendation_key = EXCLUDED.recommendation_key,\n                    beta = EXCLUDED.beta,\n                    fifty_two_week_high = EXCLUDED.fifty_two_week_high,\n                    fifty_two_week_low = EXCLUDED.fifty_two_week_low,\n                    fifty_two_week_change = EXCLUDED.fifty_two_week_change,\n                    sp500_52_week_change = EXCLUDED.sp500_52_week_change,\n                    fifty_day_average = EXCLUDED.fifty_day_average,\n                    two_hundred_day_average = EXCLUDED.two_hundred_day_average,\n                    average_volume = EXCLUDED.average_volume,\n                    average_volume_10days = EXCLUDED.average_volume_10days,\n                    regular_market_volume = EXCLUDED.regular_market_volume,\n                    last_updated = CURRENT_TIMESTAMP\n            \"\"\")\n\n            # Map metadata dict keys to database columns\n            params = {\n                'security_id': security_id,\n                'date_scraped': metadata.get('date_scraped'),\n                'company_name': metadata.get('company_name'),\n                'exchange': metadata.get('exchange'),\n                'country': metadata.get('country'),\n                'sector': metadata.get('sector'),\n                'industry': metadata.get('industry'),\n                'market_cap': metadata.get('market_cap'),\n                'enterprise_value': metadata.get('enterprise_value'),\n                'shares_outstanding': metadata.get('shares_outstanding'),\n                'float_shares': metadata.get('float_shares'),\n                'price_to_book': metadata.get('price_to_book'),\n                'forward_pe': metadata.get('forward_pe'),\n                'ev_to_ebitda': metadata.get('ev_to_ebitda'),\n                'ev_to_revenue': metadata.get('ev_to_revenue'),\n                'price_to_sales': metadata.get('price_to_sales'),\n                'gross_margin': metadata.get('gross_margin'),\n                'operating_margin': metadata.get('operating_margin'),\n                'profit_margin': metadata.get('profit_margin'),\n                'return_on_equity': metadata.get('return_on_equity'),\n                'return_on_assets': metadata.get('return_on_assets'),\n                'free_cash_flow_yield': metadata.get('free_cash_flow_yield'),\n                'revenue_growth_yoy': metadata.get('revenue_growth_yoy'),\n                'revenue_per_share': metadata.get('revenue_per_share'),\n                'debt_to_equity': metadata.get('debt_to_equity'),\n                'current_ratio': metadata.get('current_ratio'),\n                'quick_ratio': metadata.get('quick_ratio'),\n                'total_cash': metadata.get('total_cash'),\n                'total_debt': metadata.get('total_debt'),\n                'total_cash_per_share': metadata.get('total_cash_per_share'),\n                'book_value': metadata.get('book_value'),\n                'operating_cash_flow': metadata.get('operating_cash_flow'),\n                'free_cash_flow': metadata.get('free_cash_flow'),\n                'payout_ratio': metadata.get('payout_ratio'),\n                'short_percent_of_float': metadata.get('short_percent_of_float'),\n                'short_ratio': metadata.get('short_ratio'),\n                'shares_short': metadata.get('shares_short'),\n                'shares_percent_shares_out': metadata.get('shares_percent_shares_out'),\n                'held_percent_institutions': metadata.get('held_percent_institutions'),\n                'held_percent_insiders': metadata.get('held_percent_insiders'),\n                'target_mean_price': metadata.get('target_mean_price'),\n                'target_price_upside': metadata.get('target_price_upside'),\n                'number_of_analysts': metadata.get('number_of_analysts'),\n                'recommendation_key': metadata.get('recommendation_key'),\n                'beta': metadata.get('beta'),\n                # Note: scraper uses '52_week_high' but DB uses 'fifty_two_week_high'\n                'fifty_two_week_high': metadata.get('52_week_high'),\n                'fifty_two_week_low': metadata.get('52_week_low'),\n                'fifty_two_week_change': metadata.get('52_week_change'),\n                'sp500_52_week_change': metadata.get('sp500_52_week_change'),\n                'fifty_day_average': metadata.get('50_day_average'),\n                'two_hundred_day_average': metadata.get('200_day_average'),\n                'average_volume': metadata.get('average_volume'),\n                'average_volume_10days': metadata.get('average_volume_10days'),\n                'regular_market_volume': metadata.get('regular_market_volume'),\n                'data_source': metadata.get('data_source', 'yfinance')\n            }\n\n            # Execute the query\n            result = session.execute(query, params)\n\n            # Get the number of affected rows\n            rows = getattr(result, 'rowcount', 0)\n\n            session.commit()\n            return {\n                'success': True,\n                'ticker': ticker,\n                'rows_affected': rows,\n                'message': f\"Inserted metadata for {ticker}\" if rows &gt; 0 else f\"{ticker} already exists in metadata DB\"\n            }\n\n        except Exception as e:\n            session.rollback()\n            logger.error(f\"Failed to insert metadata for {ticker}: {e}\")\n            raise Exception(f\"Metadata insertion failed for {ticker}: {str(e)}\") from e\n        finally:\n            session.close()\n\n    def get_tickers(self, groupings: Optional[List[str]] = None, provider: str = 'yfinance') -&gt; List[str]:\n        \"\"\"\n        Get tickers from database, optionally filtered by groupings\n\n        Args:\n            groupings: Optional list of groupings to filter by (e.g., ['sp500', 'nasdaq'])\n                      If None, returns all tickers for the provider\n            provider: Data provider (default: 'yfinance')\n\n        Returns:\n            List of ticker symbols\n\n        Examples:\n            ```python\n            # Get all tickers\n            all_tickers = client.get_tickers()\n\n            # Get S&amp;P 500 tickers\n            sp500 = client.get_tickers(['sp500'])\n\n            # Get both S&amp;P 500 and NASDAQ tickers\n            combined = client.get_tickers(['sp500', 'nasdaq'])\n            ```\n        \"\"\"\n        session = self.Session()\n\n        try:\n            if groupings:\n\n                # Get tickers that have ANY of the specified groupings\n                query = text(\"\"\"\n                    SELECT DISTINCT ticker\n                    FROM security_master.securities\n                    WHERE provider = :provider\n                    AND groupings &amp;&amp; :groupings_array  -- Array overlap operator\n                    ORDER BY ticker\n                \"\"\")\n\n                result = session.execute(query, {\n                    'provider': provider,\n                    'groupings_array': groupings\n                })\n            else:\n\n                # Select all\n                query = text(\"\"\"\n                    SELECT ticker\n                    FROM security_master.securities\n                    WHERE provider = :provider\n                    ORDER BY ticker\n                \"\"\")\n\n                result = session.execute(query, {'provider': provider})\n\n            # Fetch tickers from result\n            tickers = [row[0] for row in result.fetchall()]\n            return tickers\n\n        except Exception as e:\n            logger.error(f\"Failed to get tickers from database: {e}\")\n            raise Exception(f\"Failed to retrieve tickers: {str(e)}\") from e\n        finally:\n            session.close()\n</code></pre>"},{"location":"data_pipeline/components/yfinance_client/#sec_master_db.clients.yfinance_client.YfinanceClient.__init__","title":"<code>__init__(db_url)</code>","text":"<p>Initialize YfinanceClient for database operations.</p> <p>Parameters:</p> Name Type Description Default <code>db_url</code> <code>str</code> <p>PostgreSQL connection string. If None, uses SEC_MASTER_DB_URL_PROD env var    or defaults to local dev database.</p> required Example <pre><code>client = YfinanceClient(\"postgresql://user:pass@localhost:5432/sec_master_dev\")\n</code></pre> Source code in <code>data_pipeline/sec_master_db/clients/yfinance_client.py</code> <pre><code>def __init__(self, db_url: str):\n    \"\"\"\n    Initialize YfinanceClient for database operations.\n\n    Args:\n        db_url: PostgreSQL connection string. If None, uses SEC_MASTER_DB_URL_PROD env var\n               or defaults to local dev database.\n\n    Example:\n        ```python\n        client = YfinanceClient(\"postgresql://user:pass@localhost:5432/sec_master_dev\")\n        ```\n    \"\"\"\n    # Set up connection\n    self.engine = create_engine(db_url)\n    self.Session = sessionmaker(bind=self.engine)\n</code></pre>"},{"location":"data_pipeline/components/yfinance_client/#sec_master_db.clients.yfinance_client.YfinanceClient.get_security_id","title":"<code>get_security_id(ticker, provider='yfinance')</code>","text":"<p>Retrieve the unique security_id for a ticker symbol.</p> <p>Parameters:</p> Name Type Description Default <code>ticker</code> <code>str</code> <p>Ticker symbol (e.g., 'AAPL')</p> required <code>provider</code> <code>str</code> <p>Data provider name (default: 'yfinance')</p> <code>'yfinance'</code> <p>Returns:</p> Name Type Description <code>int</code> <code>Optional[int]</code> <p>The security_id if found, None if ticker doesn't exist</p> Example <pre><code>security_id = client.get_security_id('AAPL')\nif security_id:\n    print(f\"AAPL has ID: {security_id}\")\n</code></pre> Note <p>This ID is used as foreign key in OHLCV and metadata tables.</p> Source code in <code>data_pipeline/sec_master_db/clients/yfinance_client.py</code> <pre><code>def get_security_id(self, ticker: str, provider: str = 'yfinance') -&gt; Optional[int]:\n    \"\"\"\n    Retrieve the unique security_id for a ticker symbol.\n\n    Args:\n        ticker: Ticker symbol (e.g., 'AAPL')\n        provider: Data provider name (default: 'yfinance')\n\n    Returns:\n        int: The security_id if found, None if ticker doesn't exist\n\n    Example:\n        ```python\n        security_id = client.get_security_id('AAPL')\n        if security_id:\n            print(f\"AAPL has ID: {security_id}\")\n        ```\n\n    Note:\n        This ID is used as foreign key in OHLCV and metadata tables.\n    \"\"\"\n    session = self.Session()\n\n    try:\n        query = text(\"\"\"\n            SELECT security_id\n            FROM security_master.securities\n            WHERE ticker = :ticker AND provider = :provider\n        \"\"\")\n\n        # Execute SQL query\n        result = session.execute(query, {'ticker': ticker, 'provider': provider})\n\n        # Get the first row\n        row = result.fetchone()\n\n        # Return the security id\n        if not row:\n            raise ValueError(f\"Security ID not found for ticker {ticker} with provider {provider}\")\n        return row[0]\n\n    except Exception as e:\n        logger.error(f\"Failed to get security_id for {ticker}: {e}\")\n        raise Exception(f\"Failed to retrieve security_id for {ticker}: {str(e)}\") from e\n    finally:\n        session.close()\n</code></pre>"},{"location":"data_pipeline/components/yfinance_client/#sec_master_db.clients.yfinance_client.YfinanceClient.get_tickers","title":"<code>get_tickers(groupings=None, provider='yfinance')</code>","text":"<p>Get tickers from database, optionally filtered by groupings</p> <p>Parameters:</p> Name Type Description Default <code>groupings</code> <code>Optional[List[str]]</code> <p>Optional list of groupings to filter by (e.g., ['sp500', 'nasdaq'])       If None, returns all tickers for the provider</p> <code>None</code> <code>provider</code> <code>str</code> <p>Data provider (default: 'yfinance')</p> <code>'yfinance'</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of ticker symbols</p> <p>Examples:</p> <pre><code># Get all tickers\nall_tickers = client.get_tickers()\n\n# Get S&amp;P 500 tickers\nsp500 = client.get_tickers(['sp500'])\n\n# Get both S&amp;P 500 and NASDAQ tickers\ncombined = client.get_tickers(['sp500', 'nasdaq'])\n</code></pre> Source code in <code>data_pipeline/sec_master_db/clients/yfinance_client.py</code> <pre><code>def get_tickers(self, groupings: Optional[List[str]] = None, provider: str = 'yfinance') -&gt; List[str]:\n    \"\"\"\n    Get tickers from database, optionally filtered by groupings\n\n    Args:\n        groupings: Optional list of groupings to filter by (e.g., ['sp500', 'nasdaq'])\n                  If None, returns all tickers for the provider\n        provider: Data provider (default: 'yfinance')\n\n    Returns:\n        List of ticker symbols\n\n    Examples:\n        ```python\n        # Get all tickers\n        all_tickers = client.get_tickers()\n\n        # Get S&amp;P 500 tickers\n        sp500 = client.get_tickers(['sp500'])\n\n        # Get both S&amp;P 500 and NASDAQ tickers\n        combined = client.get_tickers(['sp500', 'nasdaq'])\n        ```\n    \"\"\"\n    session = self.Session()\n\n    try:\n        if groupings:\n\n            # Get tickers that have ANY of the specified groupings\n            query = text(\"\"\"\n                SELECT DISTINCT ticker\n                FROM security_master.securities\n                WHERE provider = :provider\n                AND groupings &amp;&amp; :groupings_array  -- Array overlap operator\n                ORDER BY ticker\n            \"\"\")\n\n            result = session.execute(query, {\n                'provider': provider,\n                'groupings_array': groupings\n            })\n        else:\n\n            # Select all\n            query = text(\"\"\"\n                SELECT ticker\n                FROM security_master.securities\n                WHERE provider = :provider\n                ORDER BY ticker\n            \"\"\")\n\n            result = session.execute(query, {'provider': provider})\n\n        # Fetch tickers from result\n        tickers = [row[0] for row in result.fetchall()]\n        return tickers\n\n    except Exception as e:\n        logger.error(f\"Failed to get tickers from database: {e}\")\n        raise Exception(f\"Failed to retrieve tickers: {str(e)}\") from e\n    finally:\n        session.close()\n</code></pre>"},{"location":"data_pipeline/components/yfinance_client/#sec_master_db.clients.yfinance_client.YfinanceClient.insert_metadata","title":"<code>insert_metadata(ticker, metadata)</code>","text":"<p>Insert comprehensive financial metadata for a ticker.</p> <p>Parameters:</p> Name Type Description Default <code>ticker</code> <code>str</code> <p>Ticker symbol (must exist in securities table)</p> required <code>metadata</code> <code>Dict[str, Any]</code> <p>Dictionary containing financial metrics. Expected keys include:      - Company info: company_name, exchange, sector, industry, country      - Valuation: market_cap, enterprise_value, price_to_book, forward_pe      - Financials: gross_margin, operating_margin, profit_margin, debt_to_equity      - Performance: beta, 52_week_high, 52_week_low, average_volume      - And many more (see schema for full list)</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with keys: success (bool), ticker (str), rows_affected (int), message (str)</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If ticker not found or insertion fails</p> Example <pre><code>metadata = pipeline.scrape_metadata('AAPL')\nresult = client.insert_metadata('AAPL', metadata)\nprint(f\"Rows affected: {result['rows_affected']}\")\n</code></pre> Note <p>Missing keys are stored as NULL. Uses ON CONFLICT UPDATE for existing records.</p> Source code in <code>data_pipeline/sec_master_db/clients/yfinance_client.py</code> <pre><code>def insert_metadata(self, ticker: str, metadata: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Insert comprehensive financial metadata for a ticker.\n\n    Args:\n        ticker: Ticker symbol (must exist in securities table)\n        metadata: Dictionary containing financial metrics. Expected keys include:\n                 - Company info: company_name, exchange, sector, industry, country\n                 - Valuation: market_cap, enterprise_value, price_to_book, forward_pe\n                 - Financials: gross_margin, operating_margin, profit_margin, debt_to_equity\n                 - Performance: beta, 52_week_high, 52_week_low, average_volume\n                 - And many more (see schema for full list)\n\n    Returns:\n        Dict with keys: success (bool), ticker (str), rows_affected (int), message (str)\n\n    Raises:\n        Exception: If ticker not found or insertion fails\n\n    Example:\n        ```python\n        metadata = pipeline.scrape_metadata('AAPL')\n        result = client.insert_metadata('AAPL', metadata)\n        print(f\"Rows affected: {result['rows_affected']}\")\n        ```\n\n    Note:\n        Missing keys are stored as NULL. Uses ON CONFLICT UPDATE for existing records.\n    \"\"\"\n    session = self.Session()\n\n    try:\n        # Get security_id\n        security_id = self.get_security_id(ticker)\n\n        if not security_id:\n            raise Exception(f\"No security id for ticker: {ticker}\")\n\n        # Build the INSERT query with all columns\n        query = text(\"\"\"\n            INSERT INTO yfinance.stock_metadata (\n                security_id, date_scraped,\n                -- Company Basic Info\n                company_name, exchange, country, sector, industry,\n                market_cap, enterprise_value, shares_outstanding, float_shares,\n                -- Valuation Metrics\n                price_to_book, forward_pe, ev_to_ebitda, ev_to_revenue, price_to_sales,\n                -- Profitability &amp; Quality\n                gross_margin, operating_margin, profit_margin,\n                return_on_equity, return_on_assets, free_cash_flow_yield,\n                -- Growth Metrics\n                revenue_growth_yoy, revenue_per_share,\n                -- Financial Health\n                debt_to_equity, current_ratio, quick_ratio,\n                total_cash, total_debt, total_cash_per_share, book_value,\n                -- Cash Flow\n                operating_cash_flow, free_cash_flow,\n                -- Dividends\n                payout_ratio,\n                -- Short Interest &amp; Ownership\n                short_percent_of_float, short_ratio, shares_short,\n                shares_percent_shares_out, held_percent_institutions, held_percent_insiders,\n                -- Analyst Coverage\n                target_mean_price, target_price_upside, number_of_analysts, recommendation_key,\n                -- Market Performance\n                beta, fifty_two_week_high, fifty_two_week_low,\n                fifty_two_week_change, sp500_52_week_change,\n                fifty_day_average, two_hundred_day_average,\n                -- Trading Volume\n                average_volume, average_volume_10days, regular_market_volume,\n                -- Metadata\n                data_source\n            )\n            VALUES (\n                :security_id, :date_scraped,\n                -- Company Basic Info\n                :company_name, :exchange, :country, :sector, :industry,\n                :market_cap, :enterprise_value, :shares_outstanding, :float_shares,\n                -- Valuation Metrics\n                :price_to_book, :forward_pe, :ev_to_ebitda, :ev_to_revenue, :price_to_sales,\n                -- Profitability &amp; Quality\n                :gross_margin, :operating_margin, :profit_margin,\n                :return_on_equity, :return_on_assets, :free_cash_flow_yield,\n                -- Growth Metrics\n                :revenue_growth_yoy, :revenue_per_share,\n                -- Financial Health\n                :debt_to_equity, :current_ratio, :quick_ratio,\n                :total_cash, :total_debt, :total_cash_per_share, :book_value,\n                -- Cash Flow\n                :operating_cash_flow, :free_cash_flow,\n                -- Dividends\n                :payout_ratio,\n                -- Short Interest &amp; Ownership\n                :short_percent_of_float, :short_ratio, :shares_short,\n                :shares_percent_shares_out, :held_percent_institutions, :held_percent_insiders,\n                -- Analyst Coverage\n                :target_mean_price, :target_price_upside, :number_of_analysts, :recommendation_key,\n                -- Market Performance\n                :beta, :fifty_two_week_high, :fifty_two_week_low,\n                :fifty_two_week_change, :sp500_52_week_change,\n                :fifty_day_average, :two_hundred_day_average,\n                -- Trading Volume\n                :average_volume, :average_volume_10days, :regular_market_volume,\n                -- Metadata\n                :data_source\n            )\n            ON CONFLICT (security_id, date_scraped)\n            DO UPDATE SET\n                company_name = EXCLUDED.company_name,\n                exchange = EXCLUDED.exchange,\n                country = EXCLUDED.country,\n                sector = EXCLUDED.sector,\n                industry = EXCLUDED.industry,\n                market_cap = EXCLUDED.market_cap,\n                enterprise_value = EXCLUDED.enterprise_value,\n                shares_outstanding = EXCLUDED.shares_outstanding,\n                float_shares = EXCLUDED.float_shares,\n                price_to_book = EXCLUDED.price_to_book,\n                forward_pe = EXCLUDED.forward_pe,\n                ev_to_ebitda = EXCLUDED.ev_to_ebitda,\n                ev_to_revenue = EXCLUDED.ev_to_revenue,\n                price_to_sales = EXCLUDED.price_to_sales,\n                gross_margin = EXCLUDED.gross_margin,\n                operating_margin = EXCLUDED.operating_margin,\n                profit_margin = EXCLUDED.profit_margin,\n                return_on_equity = EXCLUDED.return_on_equity,\n                return_on_assets = EXCLUDED.return_on_assets,\n                free_cash_flow_yield = EXCLUDED.free_cash_flow_yield,\n                revenue_growth_yoy = EXCLUDED.revenue_growth_yoy,\n                revenue_per_share = EXCLUDED.revenue_per_share,\n                debt_to_equity = EXCLUDED.debt_to_equity,\n                current_ratio = EXCLUDED.current_ratio,\n                quick_ratio = EXCLUDED.quick_ratio,\n                total_cash = EXCLUDED.total_cash,\n                total_debt = EXCLUDED.total_debt,\n                total_cash_per_share = EXCLUDED.total_cash_per_share,\n                book_value = EXCLUDED.book_value,\n                operating_cash_flow = EXCLUDED.operating_cash_flow,\n                free_cash_flow = EXCLUDED.free_cash_flow,\n                payout_ratio = EXCLUDED.payout_ratio,\n                short_percent_of_float = EXCLUDED.short_percent_of_float,\n                short_ratio = EXCLUDED.short_ratio,\n                shares_short = EXCLUDED.shares_short,\n                shares_percent_shares_out = EXCLUDED.shares_percent_shares_out,\n                held_percent_institutions = EXCLUDED.held_percent_institutions,\n                held_percent_insiders = EXCLUDED.held_percent_insiders,\n                target_mean_price = EXCLUDED.target_mean_price,\n                target_price_upside = EXCLUDED.target_price_upside,\n                number_of_analysts = EXCLUDED.number_of_analysts,\n                recommendation_key = EXCLUDED.recommendation_key,\n                beta = EXCLUDED.beta,\n                fifty_two_week_high = EXCLUDED.fifty_two_week_high,\n                fifty_two_week_low = EXCLUDED.fifty_two_week_low,\n                fifty_two_week_change = EXCLUDED.fifty_two_week_change,\n                sp500_52_week_change = EXCLUDED.sp500_52_week_change,\n                fifty_day_average = EXCLUDED.fifty_day_average,\n                two_hundred_day_average = EXCLUDED.two_hundred_day_average,\n                average_volume = EXCLUDED.average_volume,\n                average_volume_10days = EXCLUDED.average_volume_10days,\n                regular_market_volume = EXCLUDED.regular_market_volume,\n                last_updated = CURRENT_TIMESTAMP\n        \"\"\")\n\n        # Map metadata dict keys to database columns\n        params = {\n            'security_id': security_id,\n            'date_scraped': metadata.get('date_scraped'),\n            'company_name': metadata.get('company_name'),\n            'exchange': metadata.get('exchange'),\n            'country': metadata.get('country'),\n            'sector': metadata.get('sector'),\n            'industry': metadata.get('industry'),\n            'market_cap': metadata.get('market_cap'),\n            'enterprise_value': metadata.get('enterprise_value'),\n            'shares_outstanding': metadata.get('shares_outstanding'),\n            'float_shares': metadata.get('float_shares'),\n            'price_to_book': metadata.get('price_to_book'),\n            'forward_pe': metadata.get('forward_pe'),\n            'ev_to_ebitda': metadata.get('ev_to_ebitda'),\n            'ev_to_revenue': metadata.get('ev_to_revenue'),\n            'price_to_sales': metadata.get('price_to_sales'),\n            'gross_margin': metadata.get('gross_margin'),\n            'operating_margin': metadata.get('operating_margin'),\n            'profit_margin': metadata.get('profit_margin'),\n            'return_on_equity': metadata.get('return_on_equity'),\n            'return_on_assets': metadata.get('return_on_assets'),\n            'free_cash_flow_yield': metadata.get('free_cash_flow_yield'),\n            'revenue_growth_yoy': metadata.get('revenue_growth_yoy'),\n            'revenue_per_share': metadata.get('revenue_per_share'),\n            'debt_to_equity': metadata.get('debt_to_equity'),\n            'current_ratio': metadata.get('current_ratio'),\n            'quick_ratio': metadata.get('quick_ratio'),\n            'total_cash': metadata.get('total_cash'),\n            'total_debt': metadata.get('total_debt'),\n            'total_cash_per_share': metadata.get('total_cash_per_share'),\n            'book_value': metadata.get('book_value'),\n            'operating_cash_flow': metadata.get('operating_cash_flow'),\n            'free_cash_flow': metadata.get('free_cash_flow'),\n            'payout_ratio': metadata.get('payout_ratio'),\n            'short_percent_of_float': metadata.get('short_percent_of_float'),\n            'short_ratio': metadata.get('short_ratio'),\n            'shares_short': metadata.get('shares_short'),\n            'shares_percent_shares_out': metadata.get('shares_percent_shares_out'),\n            'held_percent_institutions': metadata.get('held_percent_institutions'),\n            'held_percent_insiders': metadata.get('held_percent_insiders'),\n            'target_mean_price': metadata.get('target_mean_price'),\n            'target_price_upside': metadata.get('target_price_upside'),\n            'number_of_analysts': metadata.get('number_of_analysts'),\n            'recommendation_key': metadata.get('recommendation_key'),\n            'beta': metadata.get('beta'),\n            # Note: scraper uses '52_week_high' but DB uses 'fifty_two_week_high'\n            'fifty_two_week_high': metadata.get('52_week_high'),\n            'fifty_two_week_low': metadata.get('52_week_low'),\n            'fifty_two_week_change': metadata.get('52_week_change'),\n            'sp500_52_week_change': metadata.get('sp500_52_week_change'),\n            'fifty_day_average': metadata.get('50_day_average'),\n            'two_hundred_day_average': metadata.get('200_day_average'),\n            'average_volume': metadata.get('average_volume'),\n            'average_volume_10days': metadata.get('average_volume_10days'),\n            'regular_market_volume': metadata.get('regular_market_volume'),\n            'data_source': metadata.get('data_source', 'yfinance')\n        }\n\n        # Execute the query\n        result = session.execute(query, params)\n\n        # Get the number of affected rows\n        rows = getattr(result, 'rowcount', 0)\n\n        session.commit()\n        return {\n            'success': True,\n            'ticker': ticker,\n            'rows_affected': rows,\n            'message': f\"Inserted metadata for {ticker}\" if rows &gt; 0 else f\"{ticker} already exists in metadata DB\"\n        }\n\n    except Exception as e:\n        session.rollback()\n        logger.error(f\"Failed to insert metadata for {ticker}: {e}\")\n        raise Exception(f\"Metadata insertion failed for {ticker}: {str(e)}\") from e\n    finally:\n        session.close()\n</code></pre>"},{"location":"data_pipeline/components/yfinance_client/#sec_master_db.clients.yfinance_client.YfinanceClient.insert_ohlcv","title":"<code>insert_ohlcv(ticker, data)</code>","text":"<p>Insert OHLCV (Open, High, Low, Close, Volume) data for a single ticker.</p> <p>Parameters:</p> Name Type Description Default <code>ticker</code> <code>str</code> <p>Ticker symbol (must exist in securities table)</p> required <code>data</code> <code>DataFrame</code> <p>DataFrame with columns: Date (index), Open, High, Low, Close, Volume   Date can be either index or column.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with keys: success (bool), ticker (str), rows_affected (int), message (str)</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If ticker not found or insertion fails</p> Example <pre><code>import yfinance as yf\ndf = yf.download('AAPL', start='2024-01-01', end='2024-12-31')\nresult = client.insert_ohlcv('AAPL', df)\nprint(f\"Inserted/updated {result['rows_affected']} rows\")\n</code></pre> Note <p>Uses ON CONFLICT UPDATE to handle duplicate dates (updates existing records).</p> Source code in <code>data_pipeline/sec_master_db/clients/yfinance_client.py</code> <pre><code>def insert_ohlcv(self, ticker: str, data: pd.DataFrame) -&gt; Dict[str, Any]:\n    \"\"\"\n    Insert OHLCV (Open, High, Low, Close, Volume) data for a single ticker.\n\n    Args:\n        ticker: Ticker symbol (must exist in securities table)\n        data: DataFrame with columns: Date (index), Open, High, Low, Close, Volume\n              Date can be either index or column.\n\n    Returns:\n        Dict with keys: success (bool), ticker (str), rows_affected (int), message (str)\n\n    Raises:\n        Exception: If ticker not found or insertion fails\n\n    Example:\n        ```python\n        import yfinance as yf\n        df = yf.download('AAPL', start='2024-01-01', end='2024-12-31')\n        result = client.insert_ohlcv('AAPL', df)\n        print(f\"Inserted/updated {result['rows_affected']} rows\")\n        ```\n\n    Note:\n        Uses ON CONFLICT UPDATE to handle duplicate dates (updates existing records).\n    \"\"\"\n\n    session = self.Session()\n\n    try:\n        # Get security id \n        security_id = self.get_security_id(ticker)\n\n        # Prepare data as tuples for maximum performance\n        records = data.reset_index()\n\n        # Create list of tuples for execute_values (MUCH faster than execute_many)\n        insert_data = [\n            (\n                security_id,\n                row['Date'],\n                float(row['Open']),\n                float(row['High']),\n                float(row['Low']),\n                float(row['Close']),\n                int(row['Volume'])\n            )\n            for _, row in records.iterrows()\n        ]\n\n        # Get the raw connection from SQLAlchemy session\n        raw_conn = session.connection().connection\n        cursor = raw_conn.cursor()\n\n        # Use execute_values with ON CONFLICT for bulk upsert (SUPER FAST!)\n        execute_values(\n            cursor,\n            \"\"\"\n            INSERT INTO yfinance.ohlcv_data\n            (security_id, date, open, high, low, close, volume)\n            VALUES %s\n            ON CONFLICT (security_id, date)\n            DO UPDATE SET\n                open = EXCLUDED.open,\n                high = EXCLUDED.high,\n                low = EXCLUDED.low,\n                close = EXCLUDED.close,\n                volume = EXCLUDED.volume\n            \"\"\",\n            insert_data,\n            template=None,  # Use default template\n            page_size=1000  # Process 1000 rows at a time\n        )\n\n        # Get rows affected from cursor\n        rows = cursor.rowcount # row update/insert count\n\n        # Commit the changes\n        raw_conn.commit()\n        cursor.close()\n        return {\n            'success': True,\n            'ticker': ticker,\n            'rows_affected': rows,\n            'message': f\"Insert OHLCV for {ticker}\" if rows &gt; 0 else f\"{ticker} already in OHLCV DB\"\n        }\n\n    except Exception as e:\n        # Rollback both the raw connection and session\n        try:\n            raw_conn.rollback()\n        except:\n            pass\n        session.rollback()\n        logger.error(f\"Failed to insert OHLCV data for {ticker}: {e}\")\n        raise Exception(f\"OHLCV insertion failed for {ticker}: {str(e)}\") from e\n\n    finally:\n        session.close()\n</code></pre>"},{"location":"data_pipeline/components/yfinance_client/#sec_master_db.clients.yfinance_client.YfinanceClient.insert_security","title":"<code>insert_security(ticker, groupings, provider='yfinance')</code>","text":"<p>Insert a ticker symbol into the security_master.securities table.</p> <p>Parameters:</p> Name Type Description Default <code>ticker</code> <code>str</code> <p>Ticker symbol (e.g., 'AAPL')</p> required <code>groupings</code> <code>List[str]</code> <p>List of grouping tags (e.g., ['sp500', 'tech', 'large-cap'])</p> required <code>provider</code> <code>str</code> <p>Data provider name (default: 'yfinance')</p> <code>'yfinance'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with keys: success (bool), ticker (str), rows_affected (int), message (str)</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If database insertion fails</p> Example <pre><code>result = client.insert_security('AAPL', ['sp500', 'nasdaq'])\nif result['rows_affected'] &gt; 0:\n    print(\"Inserted successfully\")\n</code></pre> Note <p>Uses ON CONFLICT DO NOTHING, so duplicate tickers return rowcount=0.</p> Source code in <code>data_pipeline/sec_master_db/clients/yfinance_client.py</code> <pre><code>def insert_security(self, ticker: str, groupings: List[str], provider: str = 'yfinance') -&gt; Dict[str, Any]:\n    \"\"\"\n    Insert a ticker symbol into the security_master.securities table.\n\n    Args:\n        ticker: Ticker symbol (e.g., 'AAPL')\n        groupings: List of grouping tags (e.g., ['sp500', 'tech', 'large-cap'])\n        provider: Data provider name (default: 'yfinance')\n\n    Returns:\n        Dict with keys: success (bool), ticker (str), rows_affected (int), message (str)\n\n    Raises:\n        Exception: If database insertion fails\n\n    Example:\n        ```python\n        result = client.insert_security('AAPL', ['sp500', 'nasdaq'])\n        if result['rows_affected'] &gt; 0:\n            print(\"Inserted successfully\")\n        ```\n\n    Note:\n        Uses ON CONFLICT DO NOTHING, so duplicate tickers return rowcount=0.\n    \"\"\"\n\n    session = self.Session()\n\n    try:\n\n        # Insert a security\n        query = text(\"\"\"\n            INSERT INTO security_master.securities (ticker, provider, groupings, created_at)\n            VALUES (:ticker, :provider, :groupings, NOW())\n            ON CONFLICT (ticker, provider)\n            DO NOTHING\n        \"\"\")\n\n        # Execute query\n        result = session.execute(query, {'ticker': ticker, 'provider': provider, 'groupings': groupings})\n\n        # Get the number of affected rows\n        rows = getattr(result, 'rowcount', 0)\n\n        # Commit saves DB changes\n        session.commit()\n\n        # Return a formatted summary dict\n        return {\n            'success': True,\n            'ticker': ticker,\n            'rows_affected': rows,\n            'message': f\"Inserted {ticker}\" if rows &gt; 0 else f\"{ticker} already exists\"\n        }\n\n    # Catch Error\n    except Exception as e:\n        session.rollback()  # Important: rollback on error so we dont commit partial data\n        logger.error(f\"Failed to insert security {ticker}: {e}\")\n        raise Exception(f\"Security insertion failed for {ticker}: {str(e)}\") from e\n    finally:\n        session.close()\n</code></pre>"},{"location":"data_pipeline/components/yfinance_pipeline/","title":"YfinancePipeline","text":"<p>Handles OHLCV and metadata scraping from yfinance. Each method processes one ticker - Airflow does the parallelization.</p> <p>Component Separation</p> <p>Pipeline focuses on data scraping. For ticker lists see YfinanceTickers, for validation see YfinanceValidation.</p>"},{"location":"data_pipeline/components/yfinance_pipeline/#quick-start","title":"Quick Start","text":"<pre><code>from sec_data_pipeline.yfinance.yfinance_pipeline import YfinancePipeline\nfrom datetime import date\n\npipeline = YfinancePipeline()\n\n# Download OHLCV data\ndf = pipeline.scrape_date_range('AAPL', date(2024, 1, 1), date.today())\n\n# Get metadata (50+ fundamental fields)\nmeta = pipeline.scrape_metadata('AAPL')\n</code></pre>"},{"location":"data_pipeline/components/yfinance_pipeline/#data-scraping","title":"Data Scraping","text":"Type Configuration Returns OHLCV <code>auto_adjust=True</code>, flattened MultiIndex DataFrame with Date, Open, High, Low, Close, Volume Metadata 50+ fields via yfinance <code>.info</code> Dict with company info, valuation, profitability, analyst coverage <p>Adjusted Prices</p> <p><code>auto_adjust=True</code> handles stock splits and dividends automatically - no manual adjustments needed</p>"},{"location":"data_pipeline/components/yfinance_pipeline/#api-reference","title":"API Reference","text":"<p>Main data pipeline for OHLCV and metadata scraping - Atomic methods (should be bareboned &amp; simple) - No retry logic &amp; debug logging only (maybe error log) NO INFO LOGS! - Each method should download data for one stock (airflow handles parallel execution)</p> Source code in <code>data_pipeline/sec_data_pipeline/yfinance/yfinance_pipeline.py</code> <pre><code>class YfinancePipeline:\n    \"\"\"\n    Main data pipeline for OHLCV and metadata scraping\n    - Atomic methods (should be bareboned &amp; simple)\n    - No retry logic &amp; debug logging only (maybe error log) NO INFO LOGS!\n    - Each method should download data for one stock (airflow handles parallel execution)\n    \"\"\"\n\n    def scrape_date_range(\n        self,\n        ticker: str,\n        start_date: date,\n        end_date: date,\n        interval: str = '1d'\n    ) -&gt; pd.DataFrame | None:\n        \"\"\"\n        Scrape historical data for a specific date range for a single ticker\n\n        Args:\n            ticker: A single ticker symbol\n            start_date: Start date\n            end_date: End date\n            interval: Data interval (1d only)\n\n        Returns:\n            Returns a dataframe containing the ohlcv for a single stock\n        \"\"\"\n\n        # Download the data for a ticker\n        ticker_data = yf.download(\n            ticker,\n            start=start_date,\n            end=end_date,\n            interval=interval,\n            progress=False, # Disable individual progress bars\n            auto_adjust=True, # Adjusted close prices (dividends &amp; stock splits)\n        )\n\n        if ticker_data is None:\n            raise ValueError(\"OHLCV data is None\")\n\n        # Flatten MultiIndex columns (yfinance returns MultiIndex for single ticker)\n        if isinstance(ticker_data.columns, pd.MultiIndex):\n            ticker_data.columns = ticker_data.columns.get_level_values(0)\n\n        return ticker_data\n\n    def scrape_metadata(self, ticker: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Scrape fundamental metadata for a single ticker\n\n        Args:\n            ticker: A string for the ticker\n\n        Returns:\n            A dictionary with metadata about the ticker\n        \"\"\"\n\n        # Get the metadata from yfinance\n        stock = yf.Ticker(ticker)\n        info = stock.info\n\n        # Calculate derived metrics\n        current_price = info.get('currentPrice') or info.get('regularMarketPrice')\n        target_price = info.get('targetMeanPrice')\n        target_upside = None\n\n        # Calculate target upside\n        if target_price and current_price:\n            target_upside = (target_price - current_price) / current_price\n\n        # Calculate free cash flow and fcf_yield\n        free_cash_flow = info.get('freeCashflow')\n        market_cap = info.get('marketCap')\n        fcf_yield = None\n        if free_cash_flow and market_cap:\n            fcf_yield = free_cash_flow / market_cap\n\n        # Extract only high and medium availability fields (&gt;50%)\n        metadata = {\n            'ticker': ticker,\n            'date_scraped': date.today(),\n\n            # Company Basic Info (80%+ availability)\n            'company_name': info.get('longName'),\n            'exchange': info.get('exchange'),\n            'country': info.get('country'),\n            'sector': info.get('sector'),\n            'industry': info.get('industry'),\n            'market_cap': market_cap,\n            'enterprise_value': info.get('enterpriseValue'),\n            'shares_outstanding': info.get('sharesOutstanding'),\n            'float_shares': info.get('floatShares'),\n\n            # Valuation Metrics (50%+ availability)\n            'price_to_book': info.get('priceToBook'),\n            'forward_pe': info.get('forwardPE'),\n            'ev_to_ebitda': info.get('enterpriseToEbitda'),\n            'ev_to_revenue': info.get('enterpriseToRevenue'),\n            'price_to_sales': info.get('priceToSalesTrailing12Months'),\n\n            # Profitability &amp; Quality (75%+ availability)\n            'gross_margin': info.get('grossMargins'),\n            'operating_margin': info.get('operatingMargins'),\n            'profit_margin': info.get('profitMargins'),\n            'return_on_equity': info.get('returnOnEquity'),\n            'return_on_assets': info.get('returnOnAssets'),\n            'free_cash_flow_yield': fcf_yield,\n\n            # Growth Metrics (60%+ availability)\n            'revenue_growth_yoy': info.get('revenueGrowth'),\n            'revenue_per_share': info.get('revenuePerShare'),\n\n            # Financial Health (67%+ availability)\n            'debt_to_equity': info.get('debtToEquity'),\n            'current_ratio': info.get('currentRatio'),\n            'quick_ratio': info.get('quickRatio'),\n            'total_cash': info.get('totalCash'),\n            'total_debt': info.get('totalDebt'),\n            'total_cash_per_share': info.get('totalCashPerShare'),\n            'book_value': info.get('bookValue'),\n\n            # Cash Flow (77%+ availability)\n            'operating_cash_flow': info.get('operatingCashflow'),\n            'free_cash_flow': free_cash_flow,\n\n            # Dividends (81%+ availability)\n            'payout_ratio': info.get('payoutRatio'),\n\n            # Short Interest &amp; Ownership (80%+ availability)\n            'short_percent_of_float': info.get('shortPercentOfFloat'),\n            'short_ratio': info.get('shortRatio'),\n            'shares_short': info.get('sharesShort'),\n            'shares_percent_shares_out': info.get('sharesPercentSharesOut'),\n            'held_percent_institutions': info.get('heldPercentInstitutions'),\n            'held_percent_insiders': info.get('heldPercentInsiders'),\n\n            # Analyst Coverage (61%+ availability)\n            'target_mean_price': target_price,\n            'target_price_upside': target_upside,\n            'number_of_analysts': info.get('numberOfAnalystOpinions'),\n            'recommendation_key': info.get('recommendationKey'),\n\n            # Market Performance (80%+ availability)\n            'beta': info.get('beta'),\n            '52_week_high': info.get('fiftyTwoWeekHigh'),\n            '52_week_low': info.get('fiftyTwoWeekLow'),\n            '52_week_change': info.get('52WeekChange'),\n            'sp500_52_week_change': info.get('SandP52WeekChange'),\n            '50_day_average': info.get('fiftyDayAverage'),\n            '200_day_average': info.get('twoHundredDayAverage'),\n\n            # Trading Volume (100% availability)\n            'average_volume': info.get('averageVolume'),\n            'average_volume_10days': info.get('averageDailyVolume10Day'),\n            'regular_market_volume': info.get('regularMarketVolume'),\n\n            # Metadata\n            'last_updated': datetime.now(),\n            'data_source': 'yfinance'\n        }\n\n        # Return metadata\n        return metadata\n</code></pre>"},{"location":"data_pipeline/components/yfinance_pipeline/#sec_data_pipeline.yfinance.yfinance_pipeline.YfinancePipeline.scrape_date_range","title":"<code>scrape_date_range(ticker, start_date, end_date, interval='1d')</code>","text":"<p>Scrape historical data for a specific date range for a single ticker</p> <p>Parameters:</p> Name Type Description Default <code>ticker</code> <code>str</code> <p>A single ticker symbol</p> required <code>start_date</code> <code>date</code> <p>Start date</p> required <code>end_date</code> <code>date</code> <p>End date</p> required <code>interval</code> <code>str</code> <p>Data interval (1d only)</p> <code>'1d'</code> <p>Returns:</p> Type Description <code>DataFrame | None</code> <p>Returns a dataframe containing the ohlcv for a single stock</p> Source code in <code>data_pipeline/sec_data_pipeline/yfinance/yfinance_pipeline.py</code> <pre><code>def scrape_date_range(\n    self,\n    ticker: str,\n    start_date: date,\n    end_date: date,\n    interval: str = '1d'\n) -&gt; pd.DataFrame | None:\n    \"\"\"\n    Scrape historical data for a specific date range for a single ticker\n\n    Args:\n        ticker: A single ticker symbol\n        start_date: Start date\n        end_date: End date\n        interval: Data interval (1d only)\n\n    Returns:\n        Returns a dataframe containing the ohlcv for a single stock\n    \"\"\"\n\n    # Download the data for a ticker\n    ticker_data = yf.download(\n        ticker,\n        start=start_date,\n        end=end_date,\n        interval=interval,\n        progress=False, # Disable individual progress bars\n        auto_adjust=True, # Adjusted close prices (dividends &amp; stock splits)\n    )\n\n    if ticker_data is None:\n        raise ValueError(\"OHLCV data is None\")\n\n    # Flatten MultiIndex columns (yfinance returns MultiIndex for single ticker)\n    if isinstance(ticker_data.columns, pd.MultiIndex):\n        ticker_data.columns = ticker_data.columns.get_level_values(0)\n\n    return ticker_data\n</code></pre>"},{"location":"data_pipeline/components/yfinance_pipeline/#sec_data_pipeline.yfinance.yfinance_pipeline.YfinancePipeline.scrape_metadata","title":"<code>scrape_metadata(ticker)</code>","text":"<p>Scrape fundamental metadata for a single ticker</p> <p>Parameters:</p> Name Type Description Default <code>ticker</code> <code>str</code> <p>A string for the ticker</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary with metadata about the ticker</p> Source code in <code>data_pipeline/sec_data_pipeline/yfinance/yfinance_pipeline.py</code> <pre><code>def scrape_metadata(self, ticker: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Scrape fundamental metadata for a single ticker\n\n    Args:\n        ticker: A string for the ticker\n\n    Returns:\n        A dictionary with metadata about the ticker\n    \"\"\"\n\n    # Get the metadata from yfinance\n    stock = yf.Ticker(ticker)\n    info = stock.info\n\n    # Calculate derived metrics\n    current_price = info.get('currentPrice') or info.get('regularMarketPrice')\n    target_price = info.get('targetMeanPrice')\n    target_upside = None\n\n    # Calculate target upside\n    if target_price and current_price:\n        target_upside = (target_price - current_price) / current_price\n\n    # Calculate free cash flow and fcf_yield\n    free_cash_flow = info.get('freeCashflow')\n    market_cap = info.get('marketCap')\n    fcf_yield = None\n    if free_cash_flow and market_cap:\n        fcf_yield = free_cash_flow / market_cap\n\n    # Extract only high and medium availability fields (&gt;50%)\n    metadata = {\n        'ticker': ticker,\n        'date_scraped': date.today(),\n\n        # Company Basic Info (80%+ availability)\n        'company_name': info.get('longName'),\n        'exchange': info.get('exchange'),\n        'country': info.get('country'),\n        'sector': info.get('sector'),\n        'industry': info.get('industry'),\n        'market_cap': market_cap,\n        'enterprise_value': info.get('enterpriseValue'),\n        'shares_outstanding': info.get('sharesOutstanding'),\n        'float_shares': info.get('floatShares'),\n\n        # Valuation Metrics (50%+ availability)\n        'price_to_book': info.get('priceToBook'),\n        'forward_pe': info.get('forwardPE'),\n        'ev_to_ebitda': info.get('enterpriseToEbitda'),\n        'ev_to_revenue': info.get('enterpriseToRevenue'),\n        'price_to_sales': info.get('priceToSalesTrailing12Months'),\n\n        # Profitability &amp; Quality (75%+ availability)\n        'gross_margin': info.get('grossMargins'),\n        'operating_margin': info.get('operatingMargins'),\n        'profit_margin': info.get('profitMargins'),\n        'return_on_equity': info.get('returnOnEquity'),\n        'return_on_assets': info.get('returnOnAssets'),\n        'free_cash_flow_yield': fcf_yield,\n\n        # Growth Metrics (60%+ availability)\n        'revenue_growth_yoy': info.get('revenueGrowth'),\n        'revenue_per_share': info.get('revenuePerShare'),\n\n        # Financial Health (67%+ availability)\n        'debt_to_equity': info.get('debtToEquity'),\n        'current_ratio': info.get('currentRatio'),\n        'quick_ratio': info.get('quickRatio'),\n        'total_cash': info.get('totalCash'),\n        'total_debt': info.get('totalDebt'),\n        'total_cash_per_share': info.get('totalCashPerShare'),\n        'book_value': info.get('bookValue'),\n\n        # Cash Flow (77%+ availability)\n        'operating_cash_flow': info.get('operatingCashflow'),\n        'free_cash_flow': free_cash_flow,\n\n        # Dividends (81%+ availability)\n        'payout_ratio': info.get('payoutRatio'),\n\n        # Short Interest &amp; Ownership (80%+ availability)\n        'short_percent_of_float': info.get('shortPercentOfFloat'),\n        'short_ratio': info.get('shortRatio'),\n        'shares_short': info.get('sharesShort'),\n        'shares_percent_shares_out': info.get('sharesPercentSharesOut'),\n        'held_percent_institutions': info.get('heldPercentInstitutions'),\n        'held_percent_insiders': info.get('heldPercentInsiders'),\n\n        # Analyst Coverage (61%+ availability)\n        'target_mean_price': target_price,\n        'target_price_upside': target_upside,\n        'number_of_analysts': info.get('numberOfAnalystOpinions'),\n        'recommendation_key': info.get('recommendationKey'),\n\n        # Market Performance (80%+ availability)\n        'beta': info.get('beta'),\n        '52_week_high': info.get('fiftyTwoWeekHigh'),\n        '52_week_low': info.get('fiftyTwoWeekLow'),\n        '52_week_change': info.get('52WeekChange'),\n        'sp500_52_week_change': info.get('SandP52WeekChange'),\n        '50_day_average': info.get('fiftyDayAverage'),\n        '200_day_average': info.get('twoHundredDayAverage'),\n\n        # Trading Volume (100% availability)\n        'average_volume': info.get('averageVolume'),\n        'average_volume_10days': info.get('averageDailyVolume10Day'),\n        'regular_market_volume': info.get('regularMarketVolume'),\n\n        # Metadata\n        'last_updated': datetime.now(),\n        'data_source': 'yfinance'\n    }\n\n    # Return metadata\n    return metadata\n</code></pre>"},{"location":"data_pipeline/dags/daily/","title":"Daily Parallel DAG","text":"<p>Automated daily updates for all registered securities. Runs weeknights after US market close to fetch the day's OHLCV and metadata.</p>"},{"location":"data_pipeline/dags/daily/#what-it-does","title":"What It Does","text":"<p>Pulls all tickers from the database, downloads the latest trading day's data, validates it, and upserts to the hypertable. Handles weekends and holidays automatically - if you run it on Saturday, it gets Friday's data.</p> <p>Good for keeping your database current with minimal overhead.</p>"},{"location":"data_pipeline/dags/daily/#configuration","title":"Configuration","text":"Parameter Value Description <code>schedule</code> <code>'30 21 * * 1-5'</code> Weekdays 9:30 PM UTC (after US market close) <code>trigger</code> Automatic Schedule-based"},{"location":"data_pipeline/dags/daily/#task-flow","title":"Task Flow","text":"<pre><code>%%{init: {'theme':'base', 'themeVariables': { 'fontSize':'14px'}}}%%\ngraph LR\n    A[get_tickers] --&gt; B[download_ohlcv&lt;br/&gt;PARALLEL 20x]\n    B --&gt; C[validate_ohlcv&lt;br/&gt;PARALLEL 20x]\n    C --&gt; D[insert_ohlcv&lt;br/&gt;PARALLEL 20x]\n    D --&gt; E[download_metadata&lt;br/&gt;PARALLEL 20x]\n    E --&gt; F[insert_metadata&lt;br/&gt;PARALLEL 20x]\n    F --&gt; G[generate_report]\n\n    style A fill:#f0f0f0,stroke:#333,color:#000\n    style B fill:#e1f5ff,stroke:#333,color:#000\n    style C fill:#e1f5ff,stroke:#333,color:#000\n    style D fill:#e1f5ff,stroke:#333,color:#000\n    style E fill:#e1f5ff,stroke:#333,color:#000\n    style F fill:#e1f5ff,stroke:#333,color:#000\n    style G fill:#d4edda,stroke:#333,color:#000</code></pre>"},{"location":"data_pipeline/dags/daily/#parallelization","title":"Parallelization","text":"<ul> <li>Max concurrent: 20 tasks at once (<code>max_active_tis_per_dag</code>)</li> <li>Independence: Each ticker processes separately</li> <li>Failure handling: Failed tickers don't block others (<code>trigger_rule: 'all_done'</code>)</li> </ul>"},{"location":"data_pipeline/dags/daily/#retry-strategy","title":"Retry Strategy","text":"Task Retries Delay Why download_ohlcv 3 2 min yfinance rate limits validate_ohlcv 1 1 min Validation rarely fails temporarily insert_ohlcv 3 1 min DB under load download_metadata 3 2 min Network issues insert_metadata 3 1 min DB under load <p>All use exponential backoff.</p>"},{"location":"data_pipeline/dags/historical/","title":"Historical Parallel DAG","text":"<p>Bulk historical data loader. Scrapes ticker lists, validates them, downloads multi-year OHLCV data, and stores everything in the database.</p>"},{"location":"data_pipeline/dags/historical/#what-it-does","title":"What It Does","text":"<p>Gets ticker lists from public sources (Wikipedia, iShares, NASDAQ), validates each one by test-downloading recent data, then downloads the full historical range you specify. Everything runs in parallel - up to 20 tickers processing at once.</p> <p>Good for initial database setup or backfilling new ticker lists.</p>"},{"location":"data_pipeline/dags/historical/#configuration","title":"Configuration","text":"Parameter Options Default Description <code>years_back</code> 1-10 10 Years of historical data to download <code>ticker_source</code> sp500, russell3000, nasdaq, all sp500 Which ticker list to use <code>ticker_limit</code> 0-5000 0 Limit tickers (0 = no limit) <p>Trigger: Manual via Airflow UI</p>"},{"location":"data_pipeline/dags/historical/#task-flow","title":"Task Flow","text":"<pre><code>%%{init: {'theme':'base', 'themeVariables': { 'fontSize':'14px'}}}%%\ngraph LR\n    A[get_tickers] --&gt; B[convert_to_list]\n    B --&gt; C[validate_ticker&lt;br/&gt;PARALLEL 20x]\n    C --&gt; D[filter_valid]\n    D --&gt; E[register_security&lt;br/&gt;PARALLEL 20x]\n    E --&gt; F[download_ohlcv&lt;br/&gt;PARALLEL 20x]\n    F --&gt; G[validate_ohlcv&lt;br/&gt;PARALLEL 20x]\n    G --&gt; H[insert_ohlcv&lt;br/&gt;PARALLEL 20x]\n    H --&gt; I[download_metadata&lt;br/&gt;PARALLEL 20x]\n    I --&gt; J[insert_metadata&lt;br/&gt;PARALLEL 20x]\n    J --&gt; K[generate_report]\n\n    style A fill:#f0f0f0,stroke:#333,color:#000\n    style B fill:#f0f0f0,stroke:#333,color:#000\n    style C fill:#e1f5ff,stroke:#333,color:#000\n    style D fill:#f0f0f0,stroke:#333,color:#000\n    style E fill:#e1f5ff,stroke:#333,color:#000\n    style F fill:#e1f5ff,stroke:#333,color:#000\n    style G fill:#e1f5ff,stroke:#333,color:#000\n    style H fill:#e1f5ff,stroke:#333,color:#000\n    style I fill:#e1f5ff,stroke:#333,color:#000\n    style J fill:#e1f5ff,stroke:#333,color:#000\n    style K fill:#d4edda,stroke:#333,color:#000</code></pre>"},{"location":"data_pipeline/dags/historical/#parallelization","title":"Parallelization","text":"<ul> <li>Max concurrent: 20 tasks at once (<code>max_active_tis_per_dag</code>)</li> <li>Independence: Each ticker processes separately</li> <li>Failure handling: Failed tickers don't block others (<code>trigger_rule: 'all_done'</code>)</li> </ul>"},{"location":"data_pipeline/dags/historical/#retry-strategy","title":"Retry Strategy","text":"Task Retries Delay Why validate_ticker 2 1 min Network flakiness register_security 2 1 min DB connection issues download_ohlcv 3 2 min yfinance rate limits validate_ohlcv 1 1 min Validation rarely fails temporarily insert_ohlcv 3 1 min DB under load download_metadata 3 2 min Network issues insert_metadata 3 1 min DB under load <p>All use exponential backoff.</p>"},{"location":"data_pipeline/dags/historical/#example-run","title":"Example Run","text":"Small TestFull S&amp;P 500Everything <pre><code># Parameters\nticker_source = 'sp500'\nyears_back = 1\nticker_limit = 5\n\n# Result\n5 tickers \u00d7 1 year \u2248 2 minutes\n</code></pre> <pre><code># Parameters\nticker_source = 'sp500'\nyears_back = 10\nticker_limit = 0\n\n# Result\n~500 tickers \u00d7 10 years \u2248 2-3 hours\n</code></pre> <pre><code># Parameters\nticker_source = 'all'\nyears_back = 10\nticker_limit = 0\n\n# Result\n~6000 tickers \u00d7 10 years \u2248 12-18 hours\n</code></pre>"}]}